{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually building my own neural network as a learning excercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using nothing besides numpy!\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(input_layer_dims):\n",
    "    \n",
    "    # Dictionary for the parameters (weights + biases)\n",
    "    parameters = {}\n",
    "    \n",
    "    # Layer dimension length\n",
    "    length = len(input_layer_dims)\n",
    "    \n",
    "    # Loop through length\n",
    "    for i in range (1, length):\n",
    "        \n",
    "        # Initialize the weights matrix\n",
    "        parameters['W' + str(i)] = np.random.randn(input_layer_dims[i], input_layer_dims[i-1]) * 0.01\n",
    "        \n",
    "        # Initialize biases\n",
    "        parameters['b' + str(i)] = np.zeros((input_layer_dims[i], 1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[-0.00447174, -0.00319264,  0.01753352],\n",
       "        [ 0.00821962, -0.00239162,  0.0130544 ],\n",
       "        [-0.00748103,  0.00886696,  0.00384391],\n",
       "        [ 0.00256924, -0.00186721,  0.00475994],\n",
       "        [ 0.01326595, -0.00955127,  0.0095157 ]]),\n",
       " 'b1': array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initialize_parameters([3, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    return A, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9370266439430035, 2.7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(2.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    A = abs(Z * (Z > 0))\n",
    "    return A, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, -2.7)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu(-2.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function\n",
    "# A is the matrix with input data\n",
    "# W is the weights matrix\n",
    "# b is the bias\n",
    "def linear_activation(A, W, b):\n",
    "    Z = np.dot(W, A) + b\n",
    "    cache = (A, W, b)\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([16]), ([1, 2], [3, 4], [5]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_activation([1, 2], [3, 4], [5])\n",
    "# 1*3 + 2*4 + 5 = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward movement\n",
    "def forward(A_prev, W, b, activation):\n",
    "    \n",
    "    # Linear activation\n",
    "    Z, linear_cache = linear_activation(A_prev, W, b)\n",
    "    \n",
    "    # Pass through differentiable function\n",
    "    if activation == 'sigmoid':\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    elif activation == 'relu':\n",
    "        A, activation_cache = relu(Z)\n",
    "        \n",
    "    cache = (linear_cache, activation_cache) \n",
    "        \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.5]), (([-2], [3], [6]), array([0])))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward([-2], [3], [6], 'sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagation\n",
    "def forward_propagation(X, parameters):\n",
    "    \n",
    "    # List previous values (cache)\n",
    "    caches = []\n",
    "    \n",
    "    # Input data\n",
    "    A = X\n",
    "    \n",
    "    # Parameter length\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    # Loop through hidden layers\n",
    "    for i in range (1, L):\n",
    "        \n",
    "        # Store previous A value\n",
    "        A_prev = A\n",
    "        \n",
    "        # Run the forward\n",
    "        A, cache = forward(A_prev, parameters[\"W\" + str(i)], parameters[\"b\" + str(i)], activation = \"relu\")\n",
    "\n",
    "        # Save cache\n",
    "        caches.append(cache)\n",
    "        \n",
    "    # Last layer output\n",
    "    A_last, cache = forward(A, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)], activation = \"sigmoid\")\n",
    "\n",
    "    # Save cache\n",
    "    caches.append(cache)\n",
    "    \n",
    "    return (A_last, caches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Cross-Entropy Cost\n",
    "def cost_function(A_last, Y):\n",
    "    \n",
    "    # Adjust Y shape to get its length (total amount of elements)\n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    # Calculate cost comparing true value and prediction\n",
    "    cost = (-1 / m) * np.sum((Y * np.log(A_last)) + ((1 - Y) * np.log(1 - A_last)))\n",
    "    \n",
    "    # Adjust cost shape\n",
    "    cost = np.squeeze(cost)\n",
    "    \n",
    "    return (cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21072103131565256"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost_function(np.array([[0.9], [0.9]]), np.array([[1], [1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward derivative of the sigmoid function\n",
    "def sigmoid_backward(da, Z):\n",
    "    \n",
    "    # Calculate derivative of Z\n",
    "    dg = (1 / (1 + np.exp(-Z))) * (1 - (1 / (1 + np.exp(-Z))))\n",
    "    \n",
    "    # Find the change in the derivative of Z\n",
    "    dz = da * dg\n",
    "    return dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11750185610079725"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid_backward(0.5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward derivative of the relu function\n",
    "def relu_backward(da, Z):\n",
    "    \n",
    "    dg = 1 * ( Z >= 0)\n",
    "    dz = da * dg\n",
    "    return dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu_backward(0.5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear activation for back propagation\n",
    "def linear_backward_function(dz, cache):\n",
    "    \n",
    "    # Get values from cache (memory)\n",
    "    A_prev, W, b = cache\n",
    "    \n",
    "    # m shape\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    # Calculate the derivative of W (result of the operation with dz)\n",
    "    dW = (1 / m) * np.dot(dz, A_prev.T)\n",
    "    \n",
    "    # Calculate the derivative of b (result of the operation with dz)\n",
    "    db = (1 / m) * np.sum(dz, axis = 1, keepdims = True)\n",
    "\n",
    "    # Calculate the derivative of the operation\n",
    "    dA_prev = np.dot(W.T, dz)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \n",
    "    # Extract the cache\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    # Check if activation is relu\n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward_function(dZ, linear_cache)\n",
    "        \n",
    "    # Check if activation is sigmoid\n",
    "    if activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward_function(dZ, linear_cache)\n",
    "        \n",
    "    return dA_prev, dW, db   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward propagation algorithm (calculate gradients to update weights)\n",
    "# AL = predicted value during forward\n",
    "# Y = true value\n",
    "def backward_propagation(AL, Y, caches):\n",
    "    \n",
    "    # Gradients dictionary\n",
    "    grads = {}\n",
    "    \n",
    "    # Length of data (on cache)\n",
    "    L = len(caches)\n",
    "    \n",
    "    # Get the length for the m value\n",
    "    m = AL.shape[1]\n",
    "    \n",
    "    # Adjust Y shape\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    \n",
    "    # Calculate the derivative of the network's final prediction (done at the end of Forward Propagation)\n",
    "    dAL = -((Y / AL) - ((1 - Y) / (1 - AL)))\n",
    "    \n",
    "    # Get the current cache value\n",
    "    current_cache = caches[L-1]\n",
    "    \n",
    "    # Generate a list of gradients for the data, weights and biases\n",
    "    # This is done because this is the final part of the NN, starting the way back\n",
    "    # AKA this is the prediction (sigmoid) layer\n",
    "    grads[\"dA\" + str(L - 1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
    "    \n",
    "    # Loop to calculate the derivative durante the linear activations with relu\n",
    "    for l in reversed(range(L-1)):\n",
    "        \n",
    "        # Current cache\n",
    "        current_cache = caches[l]\n",
    "        \n",
    "        # Calculate derivatives\n",
    "        dA_prev, dW, db = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, activation = \"relu\")\n",
    "        \n",
    "        # Feed the gradients dictionary, using their specific indexes\n",
    "        grads[\"dA\" + str(l)] = dA_prev\n",
    "        grads[\"dW\" + str(l + 1)] = dW\n",
    "        grads[\"db\" + str(l + 1)] = db\n",
    "        \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update weights using the gradients\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \n",
    "    # Length of the dictionary with weights and biases\n",
    "    L = len(parameters)//2\n",
    "    \n",
    "    # Loop to update weights\n",
    "    for l in range(L):\n",
    "        \n",
    "        # Update weights\n",
    "        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - (learning_rate * grads[\"dW\" + str(l + 1)])\n",
    "\n",
    "        # Update biases\n",
    "        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - (learning_rate * grads[\"db\" + str(l + 1)])\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Complete Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Neural Network Model\n",
    "def NN_model(X, Y, input_layer_dims, learning_rate = 0.005, num_iterations = 20):\n",
    "    \n",
    "    # List for the cost of each training epoch\n",
    "    costs = []\n",
    "    \n",
    "    # Inicialize the parameters\n",
    "    parameters = initialize_parameters(input_layer_dims)\n",
    "    \n",
    "    # Loop for the number of epocs\n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        # Forward Propagation\n",
    "        AL, caches = forward_propagation(X, parameters)\n",
    "        \n",
    "        # Calculate cost\n",
    "        cost = cost_function(AL, Y)\n",
    "        \n",
    "        # Backward Propagation\n",
    "        gradients = backward_propagation(AL, Y, caches)\n",
    "        \n",
    "        # Update parameters\n",
    "        parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "        \n",
    "        # Print the epoch cost at every 10 epochs\n",
    "        if i % 10 == 0:\n",
    "            print('Cost after ' + str(i) + ' epochs is ' + str(cost))\n",
    "            costs.append(cost)\n",
    "            \n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, parameters):\n",
    "    AL, caches = forward_propagation(X, parameters)\n",
    "    return AL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a Preloaded Dataset to Test My NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn    0.23.1\n",
      "matplotlib 3.2.2\n",
      "numpy      1.18.5\n",
      "pandas     1.0.5\n",
      "CPython 3.7.7\n",
      "IPython 7.16.1\n"
     ]
    }
   ],
   "source": [
    "# Versões dos pacotes usados neste jupyter notebook\n",
    "%reload_ext watermark\n",
    "%watermark -v --iv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the complete dataset object on a temporary\n",
    "temp = load_breast_cancer()\n",
    "type(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01,\n",
       "         1.189e-01],\n",
       "        [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01,\n",
       "         8.902e-02],\n",
       "        [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01,\n",
       "         8.758e-02],\n",
       "        ...,\n",
       "        [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01,\n",
       "         7.820e-02],\n",
       "        [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01,\n",
       "         1.240e-01],\n",
       "        [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01,\n",
       "         7.039e-02]]),\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
       " 'frame': None,\n",
       " 'target_names': array(['malignant', 'benign'], dtype='<U9'),\n",
       " 'DESCR': '.. _breast_cancer_dataset:\\n\\nBreast cancer wisconsin (diagnostic) dataset\\n--------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 569\\n\\n    :Number of Attributes: 30 numeric, predictive attributes and the class\\n\\n    :Attribute Information:\\n        - radius (mean of distances from center to points on the perimeter)\\n        - texture (standard deviation of gray-scale values)\\n        - perimeter\\n        - area\\n        - smoothness (local variation in radius lengths)\\n        - compactness (perimeter^2 / area - 1.0)\\n        - concavity (severity of concave portions of the contour)\\n        - concave points (number of concave portions of the contour)\\n        - symmetry\\n        - fractal dimension (\"coastline approximation\" - 1)\\n\\n        The mean, standard error, and \"worst\" or largest (mean of the three\\n        worst/largest values) of these features were computed for each image,\\n        resulting in 30 features.  For instance, field 0 is Mean Radius, field\\n        10 is Radius SE, field 20 is Worst Radius.\\n\\n        - class:\\n                - WDBC-Malignant\\n                - WDBC-Benign\\n\\n    :Summary Statistics:\\n\\n    ===================================== ====== ======\\n                                           Min    Max\\n    ===================================== ====== ======\\n    radius (mean):                        6.981  28.11\\n    texture (mean):                       9.71   39.28\\n    perimeter (mean):                     43.79  188.5\\n    area (mean):                          143.5  2501.0\\n    smoothness (mean):                    0.053  0.163\\n    compactness (mean):                   0.019  0.345\\n    concavity (mean):                     0.0    0.427\\n    concave points (mean):                0.0    0.201\\n    symmetry (mean):                      0.106  0.304\\n    fractal dimension (mean):             0.05   0.097\\n    radius (standard error):              0.112  2.873\\n    texture (standard error):             0.36   4.885\\n    perimeter (standard error):           0.757  21.98\\n    area (standard error):                6.802  542.2\\n    smoothness (standard error):          0.002  0.031\\n    compactness (standard error):         0.002  0.135\\n    concavity (standard error):           0.0    0.396\\n    concave points (standard error):      0.0    0.053\\n    symmetry (standard error):            0.008  0.079\\n    fractal dimension (standard error):   0.001  0.03\\n    radius (worst):                       7.93   36.04\\n    texture (worst):                      12.02  49.54\\n    perimeter (worst):                    50.41  251.2\\n    area (worst):                         185.2  4254.0\\n    smoothness (worst):                   0.071  0.223\\n    compactness (worst):                  0.027  1.058\\n    concavity (worst):                    0.0    1.252\\n    concave points (worst):               0.0    0.291\\n    symmetry (worst):                     0.156  0.664\\n    fractal dimension (worst):            0.055  0.208\\n    ===================================== ====== ======\\n\\n    :Missing Attribute Values: None\\n\\n    :Class Distribution: 212 - Malignant, 357 - Benign\\n\\n    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\\n\\n    :Donor: Nick Street\\n\\n    :Date: November, 1995\\n\\nThis is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\\nhttps://goo.gl/U2Uwz2\\n\\nFeatures are computed from a digitized image of a fine needle\\naspirate (FNA) of a breast mass.  They describe\\ncharacteristics of the cell nuclei present in the image.\\n\\nSeparating plane described above was obtained using\\nMultisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\\nConstruction Via Linear Programming.\" Proceedings of the 4th\\nMidwest Artificial Intelligence and Cognitive Science Society,\\npp. 97-101, 1992], a classification method which uses linear\\nprogramming to construct a decision tree.  Relevant features\\nwere selected using an exhaustive search in the space of 1-4\\nfeatures and 1-3 separating planes.\\n\\nThe actual linear program used to obtain the separating plane\\nin the 3-dimensional space is that described in:\\n[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\\nProgramming Discrimination of Two Linearly Inseparable Sets\",\\nOptimization Methods and Software 1, 1992, 23-34].\\n\\nThis database is also available through the UW CS ftp server:\\n\\nftp ftp.cs.wisc.edu\\ncd math-prog/cpo-dataset/machine-learn/WDBC/\\n\\n.. topic:: References\\n\\n   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \\n     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \\n     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\\n     San Jose, CA, 1993.\\n   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \\n     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \\n     July-August 1995.\\n   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\\n     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \\n     163-171.',\n",
       " 'feature_names': array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
       "        'mean smoothness', 'mean compactness', 'mean concavity',\n",
       "        'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
       "        'radius error', 'texture error', 'perimeter error', 'area error',\n",
       "        'smoothness error', 'compactness error', 'concavity error',\n",
       "        'concave points error', 'symmetry error',\n",
       "        'fractal dimension error', 'worst radius', 'worst texture',\n",
       "        'worst perimeter', 'worst area', 'worst smoothness',\n",
       "        'worst compactness', 'worst concavity', 'worst concave points',\n",
       "        'worst symmetry', 'worst fractal dimension'], dtype='<U23'),\n",
       " 'filename': 'C:\\\\Users\\\\Matheus\\\\Anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\datasets\\\\data\\\\breast_cancer.csv'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize the object\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = pd.DataFrame(columns = load_breast_cancer()[\"feature_names\"], data = load_breast_cancer()[\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
       "0                 0.07871  ...         25.38          17.33           184.60   \n",
       "1                 0.05667  ...         24.99          23.41           158.80   \n",
       "2                 0.05999  ...         23.57          25.53           152.50   \n",
       "3                 0.09744  ...         14.91          26.50            98.87   \n",
       "4                 0.05883  ...         22.54          16.67           152.20   \n",
       "\n",
       "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   worst concave points  worst symmetry  worst fractal dimension  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mean radius                False\n",
       "mean texture               False\n",
       "mean perimeter             False\n",
       "mean area                  False\n",
       "mean smoothness            False\n",
       "mean compactness           False\n",
       "mean concavity             False\n",
       "mean concave points        False\n",
       "mean symmetry              False\n",
       "mean fractal dimension     False\n",
       "radius error               False\n",
       "texture error              False\n",
       "perimeter error            False\n",
       "area error                 False\n",
       "smoothness error           False\n",
       "compactness error          False\n",
       "concavity error            False\n",
       "concave points error       False\n",
       "symmetry error             False\n",
       "fractal dimension error    False\n",
       "worst radius               False\n",
       "worst texture              False\n",
       "worst perimeter            False\n",
       "worst area                 False\n",
       "worst smoothness           False\n",
       "worst compactness          False\n",
       "worst concavity            False\n",
       "worst concave points       False\n",
       "worst symmetry             False\n",
       "worst fractal dimension    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check of missing values\n",
    "dataset.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the target variavle\n",
    "target = load_breast_cancer()[\"target\"]\n",
    "type(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "       0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "357"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total samples per class - benign cancer\n",
    "np.count_nonzero(target == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "212"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total samples per class - malign cancer\n",
    "np.count_nonzero(target == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract labels\n",
    "\n",
    "# Labels dictionary\n",
    "labels = {}\n",
    "\n",
    "# Target variable class names\n",
    "target_names = load_breast_cancer()[\"target_names\"]\n",
    "\n",
    "# Map\n",
    "for i in range(len(target_names)):\n",
    "    labels.update({i:target_names[i]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'malignant', 1: 'benign'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01,\n",
       "        1.189e-01],\n",
       "       [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01,\n",
       "        8.902e-02],\n",
       "       [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01,\n",
       "        8.758e-02],\n",
       "       ...,\n",
       "       [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01,\n",
       "        7.820e-02],\n",
       "       [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01,\n",
       "        1.240e-01],\n",
       "       [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01,\n",
       "        7.039e-02]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare the predicting variables in X\n",
    "X = np.array(dataset)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, target, test_size = 0.15, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(483, 30)\n",
      "(483,)\n"
     ]
    }
   ],
   "source": [
    "# Shape of training data\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86, 30)\n",
      "(86,)\n"
     ]
    }
   ],
   "source": [
    "# Shape of test data\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the shape of the input data\n",
    "X_train = X_train.T\n",
    "X_test = X_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 483)\n",
      "(30, 86)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the shape of the output data\n",
    "y_train = y_train.reshape(1, len(y_train))\n",
    "y_test = y_test.reshape(1, len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 483)\n",
      "(1, 86)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[30, 50, 20, 5, 1]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Variable with the input layer dimension for the number of neurons\n",
    "input_layer_dims = [X_train.shape[0], 50, 20, 5, 1]\n",
    "input_layer_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training.\n",
      "\n",
      "Cost after 0 epochs is 0.6931304580521144\n",
      "Cost after 10 epochs is 0.6922742544292866\n",
      "Cost after 20 epochs is 0.691439026049842\n",
      "Cost after 30 epochs is 0.6906242458885682\n",
      "Cost after 40 epochs is 0.6898293979887672\n",
      "Cost after 50 epochs is 0.6890539786092041\n",
      "Cost after 60 epochs is 0.6882974930298721\n",
      "Cost after 70 epochs is 0.687559458837205\n",
      "Cost after 80 epochs is 0.6868394012599506\n",
      "Cost after 90 epochs is 0.6861368593518884\n",
      "Cost after 100 epochs is 0.6854513813098023\n",
      "Cost after 110 epochs is 0.6847825321596055\n",
      "Cost after 120 epochs is 0.6841298956889486\n",
      "Cost after 130 epochs is 0.6834930852029913\n",
      "Cost after 140 epochs is 0.6828716935310413\n",
      "Cost after 150 epochs is 0.6822653261581259\n",
      "Cost after 160 epochs is 0.6816736214933604\n",
      "Cost after 170 epochs is 0.68109618424477\n",
      "Cost after 180 epochs is 0.6805326218936708\n",
      "Cost after 190 epochs is 0.6799825677885046\n",
      "Cost after 200 epochs is 0.6794456605467839\n",
      "Cost after 210 epochs is 0.6789215491330786\n",
      "Cost after 220 epochs is 0.6784098861957498\n",
      "Cost after 230 epochs is 0.6779103313454091\n",
      "Cost after 240 epochs is 0.6774225528440888\n",
      "Cost after 250 epochs is 0.6769462235830742\n",
      "Cost after 260 epochs is 0.6764810230797204\n",
      "Cost after 270 epochs is 0.6760266325182183\n",
      "Cost after 280 epochs is 0.6755827378623259\n",
      "Cost after 290 epochs is 0.675149027355467\n",
      "Cost after 300 epochs is 0.6747251913807468\n",
      "Cost after 310 epochs is 0.6743109260305237\n",
      "Cost after 320 epochs is 0.673905924053753\n",
      "Cost after 330 epochs is 0.6735098815098284\n",
      "Cost after 340 epochs is 0.6731224930003813\n",
      "Cost after 350 epochs is 0.6727434420382505\n",
      "Cost after 360 epochs is 0.6723724258887595\n",
      "Cost after 370 epochs is 0.672009125855853\n",
      "Cost after 380 epochs is 0.6716532241929207\n",
      "Cost after 390 epochs is 0.6713044083458048\n",
      "Cost after 400 epochs is 0.6709623554393569\n",
      "Cost after 410 epochs is 0.670626766412347\n",
      "Cost after 420 epochs is 0.670297271096923\n",
      "Cost after 430 epochs is 0.6699734826677318\n",
      "Cost after 440 epochs is 0.6696549993673382\n",
      "Cost after 450 epochs is 0.6693413975549877\n",
      "Cost after 460 epochs is 0.6690322237263674\n",
      "Cost after 470 epochs is 0.6687269936479735\n",
      "Cost after 480 epochs is 0.6684251757785566\n",
      "Cost after 490 epochs is 0.6681261801449884\n",
      "Cost after 500 epochs is 0.6678293515892141\n",
      "Cost after 510 epochs is 0.6675339557908799\n",
      "Cost after 520 epochs is 0.6672391398707107\n",
      "Cost after 530 epochs is 0.6669439248349146\n",
      "Cost after 540 epochs is 0.6666471733649788\n",
      "Cost after 550 epochs is 0.6663475570728531\n",
      "Cost after 560 epochs is 0.6660434844727904\n",
      "Cost after 570 epochs is 0.6657330580703043\n",
      "Cost after 580 epochs is 0.6654139696361776\n",
      "Cost after 590 epochs is 0.6650834047605408\n",
      "Cost after 600 epochs is 0.6647378891370537\n",
      "Cost after 610 epochs is 0.6643730555867937\n",
      "Cost after 620 epochs is 0.6639833840779784\n",
      "Cost after 630 epochs is 0.6635617847265662\n",
      "Cost after 640 epochs is 0.6630990485848521\n",
      "Cost after 650 epochs is 0.6625830589970575\n",
      "Cost after 660 epochs is 0.6619976460499649\n",
      "Cost after 670 epochs is 0.6613209264871208\n",
      "Cost after 680 epochs is 0.6605228559799723\n",
      "Cost after 690 epochs is 0.6595615840913449\n",
      "Cost after 700 epochs is 0.6583781225158378\n",
      "Cost after 710 epochs is 0.6568886944530397\n",
      "Cost after 720 epochs is 0.6549741470117286\n",
      "Cost after 730 epochs is 0.6524683972706923\n",
      "Cost after 740 epochs is 0.6491565933339426\n",
      "Cost after 750 epochs is 0.6448062627197799\n",
      "Cost after 760 epochs is 0.6392858143594561\n",
      "Cost after 770 epochs is 0.6328593690555212\n",
      "Cost after 780 epochs is 0.6264649038749297\n",
      "Cost after 790 epochs is 0.621314838234475\n",
      "Cost after 800 epochs is 0.6178081320358313\n",
      "Cost after 810 epochs is 0.6153823980570488\n",
      "Cost after 820 epochs is 0.6133955208196815\n",
      "Cost after 830 epochs is 0.6115329068254801\n",
      "Cost after 840 epochs is 0.6096925495530437\n",
      "Cost after 850 epochs is 0.6078472042535569\n",
      "Cost after 860 epochs is 0.605989114096321\n",
      "Cost after 870 epochs is 0.6041110623014786\n",
      "Cost after 880 epochs is 0.6021721870879478\n",
      "Cost after 890 epochs is 0.6002548774645349\n",
      "Cost after 900 epochs is 0.5983198109501557\n",
      "Cost after 910 epochs is 0.5963591749811554\n",
      "Cost after 920 epochs is 0.5943690688861746\n",
      "Cost after 930 epochs is 0.5923452518414096\n",
      "Cost after 940 epochs is 0.5902827792902228\n",
      "Cost after 950 epochs is 0.5881758046515079\n",
      "Cost after 960 epochs is 0.586017403045497\n",
      "Cost after 970 epochs is 0.5837992587161464\n",
      "Cost after 980 epochs is 0.5815114230154753\n",
      "Cost after 990 epochs is 0.579141852937202\n",
      "Cost after 1000 epochs is 0.5766758792947242\n",
      "Cost after 1010 epochs is 0.5740954965693758\n",
      "Cost after 1020 epochs is 0.5713783924891735\n",
      "Cost after 1030 epochs is 0.5684967808205921\n",
      "Cost after 1040 epochs is 0.5654158230060281\n",
      "Cost after 1050 epochs is 0.5620916046161998\n",
      "Cost after 1060 epochs is 0.5584686076818759\n",
      "Cost after 1070 epochs is 0.5544786311039002\n",
      "Cost after 1080 epochs is 0.5500347966606582\n",
      "Cost after 1090 epochs is 0.5450329762100087\n",
      "Cost after 1100 epochs is 0.5393301431906261\n",
      "Cost after 1110 epochs is 0.5327568780464491\n",
      "Cost after 1120 epochs is 0.5251166616710337\n",
      "Cost after 1130 epochs is 0.516213173838642\n",
      "Cost after 1140 epochs is 0.5059903647681573\n",
      "Cost after 1150 epochs is 0.4945044759725293\n",
      "Cost after 1160 epochs is 0.48185297277928474\n",
      "Cost after 1170 epochs is 0.46842622996351385\n",
      "Cost after 1180 epochs is 0.4554450596127147\n",
      "Cost after 1190 epochs is 0.4439336924894468\n",
      "Cost after 1200 epochs is 0.4338783038105641\n",
      "Cost after 1210 epochs is 0.4251864330229679\n",
      "Cost after 1220 epochs is 0.41764435580033904\n",
      "Cost after 1230 epochs is 0.4109835297841426\n",
      "Cost after 1240 epochs is 0.4072763709043427\n",
      "Cost after 1250 epochs is 0.44939588619055953\n",
      "Cost after 1260 epochs is 0.4361530019634508\n",
      "Cost after 1270 epochs is 0.430991922584882\n",
      "Cost after 1280 epochs is 0.42442416666910093\n",
      "Cost after 1290 epochs is 0.41825367294384763\n",
      "Cost after 1300 epochs is 0.4092612390025749\n",
      "Cost after 1310 epochs is 0.40757240161372943\n",
      "Cost after 1320 epochs is 0.4016249214142402\n",
      "Cost after 1330 epochs is 0.3998057415771508\n",
      "Cost after 1340 epochs is 0.3976694252739425\n",
      "Cost after 1350 epochs is 0.3933970393475048\n",
      "Cost after 1360 epochs is 0.3896583118830006\n",
      "Cost after 1370 epochs is 0.3875209871731822\n",
      "Cost after 1380 epochs is 0.38637673571198444\n",
      "Cost after 1390 epochs is 0.3823389631364052\n",
      "Cost after 1400 epochs is 0.3818590852019738\n",
      "Cost after 1410 epochs is 0.379508929564215\n",
      "Cost after 1420 epochs is 0.3771502770924509\n",
      "Cost after 1430 epochs is 0.3760126682813436\n",
      "Cost after 1440 epochs is 0.37434668140011257\n",
      "Cost after 1450 epochs is 0.37026834480965437\n",
      "Cost after 1460 epochs is 0.3684838243095031\n",
      "Cost after 1470 epochs is 0.36612404218400546\n",
      "Cost after 1480 epochs is 0.36715365625381363\n",
      "Cost after 1490 epochs is 0.3650759278318123\n",
      "Cost after 1500 epochs is 0.3645618372179085\n",
      "Cost after 1510 epochs is 0.3612354181652788\n",
      "Cost after 1520 epochs is 0.3643122617333823\n",
      "Cost after 1530 epochs is 0.35636673685781367\n",
      "Cost after 1540 epochs is 0.3585433798554397\n",
      "Cost after 1550 epochs is 0.35251588230423664\n",
      "Cost after 1560 epochs is 0.3565513698937398\n",
      "Cost after 1570 epochs is 0.3509214464107927\n",
      "Cost after 1580 epochs is 0.3501900645845686\n",
      "Cost after 1590 epochs is 0.3486297849338277\n",
      "Cost after 1600 epochs is 0.34944223088611565\n",
      "Cost after 1610 epochs is 0.345540321988322\n",
      "Cost after 1620 epochs is 0.3465845703577683\n",
      "Cost after 1630 epochs is 0.34345983672092584\n",
      "Cost after 1640 epochs is 0.34345543182287336\n",
      "Cost after 1650 epochs is 0.34239382507990784\n",
      "Cost after 1660 epochs is 0.3406136293179004\n",
      "Cost after 1670 epochs is 0.3370110257678776\n",
      "Cost after 1680 epochs is 0.3311964795434893\n",
      "Cost after 1690 epochs is 0.32128102646452494\n",
      "Cost after 1700 epochs is 0.3206061143473655\n",
      "Cost after 1710 epochs is 0.31983084043374205\n",
      "Cost after 1720 epochs is 0.32049348729499666\n",
      "Cost after 1730 epochs is 0.3183576730029515\n",
      "Cost after 1740 epochs is 0.317910856511288\n",
      "Cost after 1750 epochs is 0.3166116657183696\n",
      "Cost after 1760 epochs is 0.3182659918694704\n",
      "Cost after 1770 epochs is 0.320619424117694\n",
      "Cost after 1780 epochs is 0.3415741569604561\n",
      "Cost after 1790 epochs is 0.33125226271084895\n",
      "Cost after 1800 epochs is 0.33584234356085557\n",
      "Cost after 1810 epochs is 0.3311269709535801\n",
      "Cost after 1820 epochs is 0.3307563619354851\n",
      "Cost after 1830 epochs is 0.32796958041672003\n",
      "Cost after 1840 epochs is 0.3221143927738398\n",
      "Cost after 1850 epochs is 0.319197854775166\n",
      "Cost after 1860 epochs is 0.3181477942231375\n",
      "Cost after 1870 epochs is 0.317314860593556\n",
      "Cost after 1880 epochs is 0.3176902438869636\n",
      "Cost after 1890 epochs is 0.3161432457306157\n",
      "Cost after 1900 epochs is 0.3145766246361672\n",
      "Cost after 1910 epochs is 0.3144304745680433\n",
      "Cost after 1920 epochs is 0.31274198790978436\n",
      "Cost after 1930 epochs is 0.3095427348630494\n",
      "Cost after 1940 epochs is 0.30110378155222606\n",
      "Cost after 1950 epochs is 0.30011549763440265\n",
      "Cost after 1960 epochs is 0.29878709361146855\n",
      "Cost after 1970 epochs is 0.300435196534384\n",
      "Cost after 1980 epochs is 0.30013935343253234\n",
      "Cost after 1990 epochs is 0.30009343099820385\n",
      "Cost after 2000 epochs is 0.30098255015586045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after 2010 epochs is 0.3009518198125595\n",
      "Cost after 2020 epochs is 0.29847526099599475\n",
      "Cost after 2030 epochs is 0.2993621215442026\n",
      "Cost after 2040 epochs is 0.29751224101669466\n",
      "Cost after 2050 epochs is 0.296830043357596\n",
      "Cost after 2060 epochs is 0.29909385152101714\n",
      "Cost after 2070 epochs is 0.29637892296238194\n",
      "Cost after 2080 epochs is 0.2978172683643826\n",
      "Cost after 2090 epochs is 0.29794313935075023\n",
      "Cost after 2100 epochs is 0.29396979806393114\n",
      "Cost after 2110 epochs is 0.29436499676437095\n",
      "Cost after 2120 epochs is 0.29612124194357775\n",
      "Cost after 2130 epochs is 0.292518317189885\n",
      "Cost after 2140 epochs is 0.2983768134352726\n",
      "Cost after 2150 epochs is 0.29406169908429103\n",
      "Cost after 2160 epochs is 0.29283117683842624\n",
      "Cost after 2170 epochs is 0.291067834690254\n",
      "Cost after 2180 epochs is 0.2913954500111914\n",
      "Cost after 2190 epochs is 0.2919154513257292\n",
      "Cost after 2200 epochs is 0.28996990107342846\n",
      "Cost after 2210 epochs is 0.2897778096663196\n",
      "Cost after 2220 epochs is 0.2913115012288931\n",
      "Cost after 2230 epochs is 0.2887179387620864\n",
      "Cost after 2240 epochs is 0.29131671198719544\n",
      "Cost after 2250 epochs is 0.2876087384719468\n",
      "Cost after 2260 epochs is 0.2893378025031119\n",
      "Cost after 2270 epochs is 0.28952758842806847\n",
      "Cost after 2280 epochs is 0.2889340429520992\n",
      "Cost after 2290 epochs is 0.28640125073370815\n",
      "Cost after 2300 epochs is 0.28960722226960167\n",
      "Cost after 2310 epochs is 0.2902203613483251\n",
      "Cost after 2320 epochs is 0.28968893504846355\n",
      "Cost after 2330 epochs is 0.2879516074294169\n",
      "Cost after 2340 epochs is 0.2861677523463038\n",
      "Cost after 2350 epochs is 0.2832196479246027\n",
      "Cost after 2360 epochs is 0.28489799919493236\n",
      "Cost after 2370 epochs is 0.28361575331303746\n",
      "Cost after 2380 epochs is 0.28391984323679215\n",
      "Cost after 2390 epochs is 0.28248207085632987\n",
      "Cost after 2400 epochs is 0.28241930546326877\n",
      "Cost after 2410 epochs is 0.28144785798441635\n",
      "Cost after 2420 epochs is 0.2826505828140209\n",
      "Cost after 2430 epochs is 0.2832488113646631\n",
      "Cost after 2440 epochs is 0.27953002669718535\n",
      "Cost after 2450 epochs is 0.27586980535823064\n",
      "Cost after 2460 epochs is 0.2795835649436827\n",
      "Cost after 2470 epochs is 0.28095593374078726\n",
      "Cost after 2480 epochs is 0.27935837278920095\n",
      "Cost after 2490 epochs is 0.28044868239101295\n",
      "Cost after 2500 epochs is 0.27811298559375974\n",
      "Cost after 2510 epochs is 0.27849425583498655\n",
      "Cost after 2520 epochs is 0.27745642909924173\n",
      "Cost after 2530 epochs is 0.2763896735325652\n",
      "Cost after 2540 epochs is 0.2727075064190929\n",
      "Cost after 2550 epochs is 0.27293654369774484\n",
      "Cost after 2560 epochs is 0.2743128249727928\n",
      "Cost after 2570 epochs is 0.2719852180044934\n",
      "Cost after 2580 epochs is 0.27272241144147347\n",
      "Cost after 2590 epochs is 0.27235621699323215\n",
      "Cost after 2600 epochs is 0.27119089685613373\n",
      "Cost after 2610 epochs is 0.27180632319976733\n",
      "Cost after 2620 epochs is 0.2702452758589713\n",
      "Cost after 2630 epochs is 0.2707337117133918\n",
      "Cost after 2640 epochs is 0.27054056776563346\n",
      "Cost after 2650 epochs is 0.2697898091615516\n",
      "Cost after 2660 epochs is 0.26976753368339756\n",
      "Cost after 2670 epochs is 0.2678418121717704\n",
      "Cost after 2680 epochs is 0.2723735636254019\n",
      "Cost after 2690 epochs is 0.2716940698092063\n",
      "Cost after 2700 epochs is 0.27034278564417796\n",
      "Cost after 2710 epochs is 0.2694194743345172\n",
      "Cost after 2720 epochs is 0.2705413856356412\n",
      "Cost after 2730 epochs is 0.2690176154713404\n",
      "Cost after 2740 epochs is 0.26874559451964586\n",
      "Cost after 2750 epochs is 0.2668801411257349\n",
      "Cost after 2760 epochs is 0.2667870980447782\n",
      "Cost after 2770 epochs is 0.26604902116920115\n",
      "Cost after 2780 epochs is 0.2623136862597268\n",
      "Cost after 2790 epochs is 0.26676638979749456\n",
      "Cost after 2800 epochs is 0.26403702745573643\n",
      "Cost after 2810 epochs is 0.26298954959324405\n",
      "Cost after 2820 epochs is 0.26422183622363166\n",
      "Cost after 2830 epochs is 0.2638103986980101\n",
      "Cost after 2840 epochs is 0.2625947852670944\n",
      "Cost after 2850 epochs is 0.2622815053346211\n",
      "Cost after 2860 epochs is 0.26226216537189223\n",
      "Cost after 2870 epochs is 0.25966425499103835\n",
      "Cost after 2880 epochs is 0.26216907671310297\n",
      "Cost after 2890 epochs is 0.2616879146569248\n",
      "Cost after 2900 epochs is 0.26365288091274736\n",
      "Cost after 2910 epochs is 0.2577205614476418\n",
      "Cost after 2920 epochs is 0.25839627219619227\n",
      "Cost after 2930 epochs is 0.259028607203608\n",
      "Cost after 2940 epochs is 0.2587986973265977\n",
      "Cost after 2950 epochs is 0.2563472152759951\n",
      "Cost after 2960 epochs is 0.25775881775077086\n",
      "Cost after 2970 epochs is 0.25771469394227703\n",
      "Cost after 2980 epochs is 0.25551010342613356\n",
      "Cost after 2990 epochs is 0.2555665229200765\n",
      "Cost after 3000 epochs is 0.25507493339932125\n",
      "Cost after 3010 epochs is 0.2540624310981445\n",
      "Cost after 3020 epochs is 0.2564354806875036\n",
      "Cost after 3030 epochs is 0.25594014490852296\n",
      "Cost after 3040 epochs is 0.2561776674050344\n",
      "Cost after 3050 epochs is 0.2564395826497092\n",
      "Cost after 3060 epochs is 0.25426644844128016\n",
      "Cost after 3070 epochs is 0.2515053375732044\n",
      "Cost after 3080 epochs is 0.2539517198809962\n",
      "Cost after 3090 epochs is 0.2551386325696129\n",
      "Cost after 3100 epochs is 0.25247953055641353\n",
      "Cost after 3110 epochs is 0.2482079058632544\n",
      "Cost after 3120 epochs is 0.24830359110218989\n",
      "Cost after 3130 epochs is 0.24810942704120753\n",
      "Cost after 3140 epochs is 0.2475310626147396\n",
      "Cost after 3150 epochs is 0.2469931240347489\n",
      "Cost after 3160 epochs is 0.2476231619092826\n",
      "Cost after 3170 epochs is 0.24830129052180616\n",
      "Cost after 3180 epochs is 0.2536080488968158\n",
      "Cost after 3190 epochs is 0.2545927234445736\n",
      "Cost after 3200 epochs is 0.25363666913653843\n",
      "Cost after 3210 epochs is 0.252892094991828\n",
      "Cost after 3220 epochs is 0.2506602919880666\n",
      "Cost after 3230 epochs is 0.2524674652886548\n",
      "Cost after 3240 epochs is 0.2515836366155972\n",
      "Cost after 3250 epochs is 0.25212094485239406\n",
      "Cost after 3260 epochs is 0.2526080993886427\n",
      "Cost after 3270 epochs is 0.2506144839034108\n",
      "Cost after 3280 epochs is 0.25007564875919075\n",
      "Cost after 3290 epochs is 0.25168239789903935\n",
      "Cost after 3300 epochs is 0.2506189380792863\n",
      "Cost after 3310 epochs is 0.24999455824800168\n",
      "Cost after 3320 epochs is 0.25037000055865005\n",
      "Cost after 3330 epochs is 0.24998781182238247\n",
      "Cost after 3340 epochs is 0.24988647224672988\n",
      "Cost after 3350 epochs is 0.24910604449249393\n",
      "Cost after 3360 epochs is 0.2476519730315126\n",
      "Cost after 3370 epochs is 0.24194519204161555\n",
      "Cost after 3380 epochs is 0.24231745579311154\n",
      "Cost after 3390 epochs is 0.242068271930711\n",
      "Cost after 3400 epochs is 0.24254147042100027\n",
      "Cost after 3410 epochs is 0.24227524262054378\n",
      "Cost after 3420 epochs is 0.24251586708122747\n",
      "Cost after 3430 epochs is 0.2421581660148219\n",
      "Cost after 3440 epochs is 0.24131302088456236\n",
      "Cost after 3450 epochs is 0.24134113930907422\n",
      "Cost after 3460 epochs is 0.24103176927163733\n",
      "Cost after 3470 epochs is 0.2407724383186343\n",
      "Cost after 3480 epochs is 0.24068166499949792\n",
      "Cost after 3490 epochs is 0.24083804464941705\n",
      "Cost after 3500 epochs is 0.2404345573489283\n",
      "Cost after 3510 epochs is 0.24700939344803022\n",
      "Cost after 3520 epochs is 0.24819432375756584\n",
      "Cost after 3530 epochs is 0.2462665682395585\n",
      "Cost after 3540 epochs is 0.2452987992880495\n",
      "Cost after 3550 epochs is 0.24612018315241713\n",
      "Cost after 3560 epochs is 0.24419256404512443\n",
      "Cost after 3570 epochs is 0.241866133337166\n",
      "Cost after 3580 epochs is 0.23750337953952927\n",
      "Cost after 3590 epochs is 0.23408058069828838\n",
      "Cost after 3600 epochs is 0.24498248907410247\n",
      "Cost after 3610 epochs is 0.23994681850901825\n",
      "Cost after 3620 epochs is 0.238626482579778\n",
      "Cost after 3630 epochs is 0.23990664246769658\n",
      "Cost after 3640 epochs is 0.2410473906315275\n",
      "Cost after 3650 epochs is 0.2386197038718208\n",
      "Cost after 3660 epochs is 0.23910553992791456\n",
      "Cost after 3670 epochs is 0.23946544576172685\n",
      "Cost after 3680 epochs is 0.23434518133928908\n",
      "Cost after 3690 epochs is 0.23393822100305367\n",
      "Cost after 3700 epochs is 0.23518295713205267\n",
      "Cost after 3710 epochs is 0.23470445276448187\n",
      "Cost after 3720 epochs is 0.23924070575397305\n",
      "Cost after 3730 epochs is 0.24284714115238604\n",
      "Cost after 3740 epochs is 0.24077719031106604\n",
      "Cost after 3750 epochs is 0.24159373816088042\n",
      "Cost after 3760 epochs is 0.24112771140825778\n",
      "Cost after 3770 epochs is 0.23842979326060523\n",
      "Cost after 3780 epochs is 0.2355156925831009\n",
      "Cost after 3790 epochs is 0.23535666362511892\n",
      "Cost after 3800 epochs is 0.23683290081602054\n",
      "Cost after 3810 epochs is 0.23694827445793917\n",
      "Cost after 3820 epochs is 0.23583462293856075\n",
      "Cost after 3830 epochs is 0.23001836170959844\n",
      "Cost after 3840 epochs is 0.23547378023305215\n",
      "Cost after 3850 epochs is 0.23853157036769165\n",
      "Cost after 3860 epochs is 0.23765644302102049\n",
      "Cost after 3870 epochs is 0.23456627018965895\n",
      "Cost after 3880 epochs is 0.23736120396679178\n",
      "Cost after 3890 epochs is 0.23128808381658636\n",
      "Cost after 3900 epochs is 0.22849159712549236\n",
      "Cost after 3910 epochs is 0.23698873411031549\n",
      "Cost after 3920 epochs is 0.23740219901583987\n",
      "Cost after 3930 epochs is 0.23222640239416997\n",
      "Cost after 3940 epochs is 0.23215538952910245\n",
      "Cost after 3950 epochs is 0.23283709148274662\n",
      "Cost after 3960 epochs is 0.2327636186060637\n",
      "Cost after 3970 epochs is 0.23315250840508508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after 3980 epochs is 0.22924667680746755\n",
      "Cost after 3990 epochs is 0.22562820634633315\n",
      "Cost after 4000 epochs is 0.22865562294712546\n",
      "Cost after 4010 epochs is 0.23563096852160395\n",
      "Cost after 4020 epochs is 0.23243768921796934\n",
      "Cost after 4030 epochs is 0.22957986538171732\n",
      "Cost after 4040 epochs is 0.22957229610249882\n",
      "Cost after 4050 epochs is 0.22848573520835316\n",
      "Cost after 4060 epochs is 0.2247858446157539\n",
      "Cost after 4070 epochs is 0.23336410821914377\n",
      "Cost after 4080 epochs is 0.2317396527270898\n",
      "Cost after 4090 epochs is 0.22774607891293794\n",
      "Cost after 4100 epochs is 0.22559517440525564\n",
      "Cost after 4110 epochs is 0.23187898665139012\n",
      "Cost after 4120 epochs is 0.23579791096569006\n",
      "Cost after 4130 epochs is 0.2295948869571262\n",
      "Cost after 4140 epochs is 0.22584709224178276\n",
      "Cost after 4150 epochs is 0.22909127754922368\n",
      "Cost after 4160 epochs is 0.2333114796334365\n",
      "Cost after 4170 epochs is 0.22592427338390658\n",
      "Cost after 4180 epochs is 0.22749264558080615\n",
      "Cost after 4190 epochs is 0.23385854840327114\n",
      "Cost after 4200 epochs is 0.2253129852448298\n",
      "Cost after 4210 epochs is 0.22257344168823812\n",
      "Cost after 4220 epochs is 0.2316884422723068\n",
      "Cost after 4230 epochs is 0.23004265031691457\n",
      "Cost after 4240 epochs is 0.22445695939042998\n",
      "Cost after 4250 epochs is 0.22658895064949097\n",
      "Cost after 4260 epochs is 0.22986253578816393\n",
      "Cost after 4270 epochs is 0.23044064156381472\n",
      "Cost after 4280 epochs is 0.22947704220130508\n",
      "Cost after 4290 epochs is 0.22965826970121012\n",
      "Cost after 4300 epochs is 0.23026590738976455\n",
      "Cost after 4310 epochs is 0.22912179384131146\n",
      "Cost after 4320 epochs is 0.2284799448009291\n",
      "Cost after 4330 epochs is 0.22413454222156837\n",
      "Cost after 4340 epochs is 0.22174692894536763\n",
      "Cost after 4350 epochs is 0.22579090008126437\n",
      "Cost after 4360 epochs is 0.22760494053770197\n",
      "Cost after 4370 epochs is 0.2267942216021744\n",
      "Cost after 4380 epochs is 0.22692003878610648\n",
      "Cost after 4390 epochs is 0.2258955131146606\n",
      "Cost after 4400 epochs is 0.22301045264042396\n",
      "Cost after 4410 epochs is 0.221616507728844\n",
      "Cost after 4420 epochs is 0.22217093939457452\n",
      "Cost after 4430 epochs is 0.2225816002157424\n",
      "Cost after 4440 epochs is 0.22416320423556527\n",
      "Cost after 4450 epochs is 0.22190328481786192\n",
      "Cost after 4460 epochs is 0.2218933337693835\n",
      "Cost after 4470 epochs is 0.22297599255862835\n",
      "Cost after 4480 epochs is 0.22349555440500776\n",
      "Cost after 4490 epochs is 0.22576812584602238\n",
      "Cost after 4500 epochs is 0.2251838266137034\n",
      "Cost after 4510 epochs is 0.2249720233660468\n",
      "Cost after 4520 epochs is 0.22552926119648797\n",
      "Cost after 4530 epochs is 0.22058146275290713\n",
      "Cost after 4540 epochs is 0.22149550574082646\n",
      "Cost after 4550 epochs is 0.22141283996758418\n",
      "Cost after 4560 epochs is 0.21946485651617903\n",
      "Cost after 4570 epochs is 0.22137988751570287\n",
      "Cost after 4580 epochs is 0.22016126086838927\n",
      "Cost after 4590 epochs is 0.22126968381074208\n",
      "Cost after 4600 epochs is 0.2209210329690645\n",
      "Cost after 4610 epochs is 0.22208525283639338\n",
      "Cost after 4620 epochs is 0.2209254046861853\n",
      "Cost after 4630 epochs is 0.22156139167762273\n",
      "Cost after 4640 epochs is 0.21987841842460767\n",
      "Cost after 4650 epochs is 0.22069401936234817\n",
      "Cost after 4660 epochs is 0.21929234094365735\n",
      "Cost after 4670 epochs is 0.21936006785662396\n",
      "Cost after 4680 epochs is 0.21875075615637588\n",
      "Cost after 4690 epochs is 0.21962187257517046\n",
      "Cost after 4700 epochs is 0.2208172089171569\n",
      "Cost after 4710 epochs is 0.2217960751507673\n",
      "Cost after 4720 epochs is 0.21683452247223076\n",
      "Cost after 4730 epochs is 0.2174046326160635\n",
      "Cost after 4740 epochs is 0.21908805915229704\n",
      "Cost after 4750 epochs is 0.21837690601049506\n",
      "Cost after 4760 epochs is 0.21841073121224236\n",
      "Cost after 4770 epochs is 0.21785085981292163\n",
      "Cost after 4780 epochs is 0.21819706632980845\n",
      "Cost after 4790 epochs is 0.21824736239119175\n",
      "Cost after 4800 epochs is 0.21811095593208762\n",
      "Cost after 4810 epochs is 0.21800606638127012\n",
      "Cost after 4820 epochs is 0.217375608379621\n",
      "Cost after 4830 epochs is 0.21763636726613556\n",
      "Cost after 4840 epochs is 0.21566271032921855\n",
      "Cost after 4850 epochs is 0.21735111779613997\n",
      "Cost after 4860 epochs is 0.2185387797439712\n",
      "Cost after 4870 epochs is 0.21758425163435396\n",
      "Cost after 4880 epochs is 0.21414383525940475\n",
      "Cost after 4890 epochs is 0.21090524344979153\n",
      "Cost after 4900 epochs is 0.21185664071208268\n",
      "Cost after 4910 epochs is 0.21345265891614787\n",
      "Cost after 4920 epochs is 0.2216868519638286\n",
      "Cost after 4930 epochs is 0.21690852961913826\n",
      "Cost after 4940 epochs is 0.21717777748917408\n",
      "Cost after 4950 epochs is 0.21681892678230993\n",
      "Cost after 4960 epochs is 0.2153104174121715\n",
      "Cost after 4970 epochs is 0.21186277421315278\n",
      "Cost after 4980 epochs is 0.2107815464155775\n",
      "Cost after 4990 epochs is 0.21434714520602646\n",
      "Cost after 5000 epochs is 0.21836154402504426\n",
      "Cost after 5010 epochs is 0.2150451816846959\n",
      "Cost after 5020 epochs is 0.21732755266115883\n",
      "Cost after 5030 epochs is 0.21599961707943652\n",
      "Cost after 5040 epochs is 0.2146077009198641\n",
      "Cost after 5050 epochs is 0.21436915355937153\n",
      "Cost after 5060 epochs is 0.2126715715204942\n",
      "Cost after 5070 epochs is 0.208364744666893\n",
      "Cost after 5080 epochs is 0.20795128747553623\n",
      "Cost after 5090 epochs is 0.20885597494444014\n",
      "Cost after 5100 epochs is 0.2131077241720419\n",
      "Cost after 5110 epochs is 0.2153619112865813\n",
      "Cost after 5120 epochs is 0.21577394201594505\n",
      "Cost after 5130 epochs is 0.21442209266873552\n",
      "Cost after 5140 epochs is 0.21428608274835856\n",
      "Cost after 5150 epochs is 0.21133460166247622\n",
      "Cost after 5160 epochs is 0.2070495471437771\n",
      "Cost after 5170 epochs is 0.20692687179949687\n",
      "Cost after 5180 epochs is 0.20717372448618862\n",
      "Cost after 5190 epochs is 0.20598921774991133\n",
      "Cost after 5200 epochs is 0.20711994057716762\n",
      "Cost after 5210 epochs is 0.21089696865398824\n",
      "Cost after 5220 epochs is 0.22043891711555103\n",
      "Cost after 5230 epochs is 0.21345790642366932\n",
      "Cost after 5240 epochs is 0.2123752264695368\n",
      "Cost after 5250 epochs is 0.21215802730067065\n",
      "Cost after 5260 epochs is 0.21059973030204238\n",
      "Cost after 5270 epochs is 0.21132714877199046\n",
      "Cost after 5280 epochs is 0.20976626468619888\n",
      "Cost after 5290 epochs is 0.20742520418276675\n",
      "Cost after 5300 epochs is 0.20695439396500642\n",
      "Cost after 5310 epochs is 0.20581944108469946\n",
      "Cost after 5320 epochs is 0.20614760664055273\n",
      "Cost after 5330 epochs is 0.20941310102677804\n",
      "Cost after 5340 epochs is 0.2111152630384859\n",
      "Cost after 5350 epochs is 0.2098189693550647\n",
      "Cost after 5360 epochs is 0.20777476880358597\n",
      "Cost after 5370 epochs is 0.20642210084832474\n",
      "Cost after 5380 epochs is 0.2073830676218139\n",
      "Cost after 5390 epochs is 0.21019545025404096\n",
      "Cost after 5400 epochs is 0.21584996513639204\n",
      "Cost after 5410 epochs is 0.2124295959364984\n",
      "Cost after 5420 epochs is 0.21512351556615555\n",
      "Cost after 5430 epochs is 0.21095781033327027\n",
      "Cost after 5440 epochs is 0.20974178750621397\n",
      "Cost after 5450 epochs is 0.20934620867881779\n",
      "Cost after 5460 epochs is 0.20862508010284062\n",
      "Cost after 5470 epochs is 0.21080189311630393\n",
      "Cost after 5480 epochs is 0.20868923985579274\n",
      "Cost after 5490 epochs is 0.2093096147867125\n",
      "Cost after 5500 epochs is 0.21137614006205283\n",
      "Cost after 5510 epochs is 0.209027838227749\n",
      "Cost after 5520 epochs is 0.20933977067311393\n",
      "Cost after 5530 epochs is 0.20812669258175825\n",
      "Cost after 5540 epochs is 0.20833844862824527\n",
      "Cost after 5550 epochs is 0.21037623653096263\n",
      "Cost after 5560 epochs is 0.20714864429092886\n",
      "Cost after 5570 epochs is 0.2046342455193824\n",
      "Cost after 5580 epochs is 0.2030596201135063\n",
      "Cost after 5590 epochs is 0.2039411168229471\n",
      "Cost after 5600 epochs is 0.20649316041157026\n",
      "Cost after 5610 epochs is 0.20878201573074398\n",
      "Cost after 5620 epochs is 0.2078772935044017\n",
      "Cost after 5630 epochs is 0.20732539081672954\n",
      "Cost after 5640 epochs is 0.2085567327664599\n",
      "Cost after 5650 epochs is 0.20706433414079953\n",
      "Cost after 5660 epochs is 0.20841004312516626\n",
      "Cost after 5670 epochs is 0.2059257036779992\n",
      "Cost after 5680 epochs is 0.20709289972781\n",
      "Cost after 5690 epochs is 0.20837833037328854\n",
      "Cost after 5700 epochs is 0.20595576653045644\n",
      "Cost after 5710 epochs is 0.20442887914952526\n",
      "Cost after 5720 epochs is 0.20186294828342824\n",
      "Cost after 5730 epochs is 0.2023100506591138\n",
      "Cost after 5740 epochs is 0.201436922523617\n",
      "Cost after 5750 epochs is 0.20624510182380973\n",
      "Cost after 5760 epochs is 0.20956940589284934\n",
      "Cost after 5770 epochs is 0.21071381833101663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after 5780 epochs is 0.20583116419224667\n",
      "Cost after 5790 epochs is 0.2046258309508361\n",
      "Cost after 5800 epochs is 0.20383684031495927\n",
      "Cost after 5810 epochs is 0.20680558597620996\n",
      "Cost after 5820 epochs is 0.2092816216600948\n",
      "Cost after 5830 epochs is 0.20601276808424865\n",
      "Cost after 5840 epochs is 0.20368028155410814\n",
      "Cost after 5850 epochs is 0.20514259763230516\n",
      "Cost after 5860 epochs is 0.20447729278487684\n",
      "Cost after 5870 epochs is 0.20277661895977464\n",
      "Cost after 5880 epochs is 0.20433671282345497\n",
      "Cost after 5890 epochs is 0.20474325357029882\n",
      "Cost after 5900 epochs is 0.2024319035490838\n",
      "Cost after 5910 epochs is 0.20281280463368237\n",
      "Cost after 5920 epochs is 0.20323621806559716\n",
      "Cost after 5930 epochs is 0.20290271974404625\n",
      "Cost after 5940 epochs is 0.2035747158058361\n",
      "Cost after 5950 epochs is 0.20304054577816735\n",
      "Cost after 5960 epochs is 0.2040730730505779\n",
      "Cost after 5970 epochs is 0.20426076589586423\n",
      "Cost after 5980 epochs is 0.2030701076825719\n",
      "Cost after 5990 epochs is 0.2029069963109929\n",
      "Cost after 6000 epochs is 0.20253770850052638\n",
      "Cost after 6010 epochs is 0.20435543398605224\n",
      "Cost after 6020 epochs is 0.2019761859204743\n",
      "Cost after 6030 epochs is 0.20382103868983417\n",
      "Cost after 6040 epochs is 0.20288705707756888\n",
      "Cost after 6050 epochs is 0.2044096694020846\n",
      "Cost after 6060 epochs is 0.20483213169908457\n",
      "Cost after 6070 epochs is 0.20571166167168764\n",
      "Cost after 6080 epochs is 0.2071449745860062\n",
      "Cost after 6090 epochs is 0.20708093455992957\n",
      "Cost after 6100 epochs is 0.20772096012024946\n",
      "Cost after 6110 epochs is 0.20452208313423156\n",
      "Cost after 6120 epochs is 0.20059873825178645\n",
      "Cost after 6130 epochs is 0.20128237612353608\n",
      "Cost after 6140 epochs is 0.2022323520347071\n",
      "Cost after 6150 epochs is 0.2015589381527689\n",
      "Cost after 6160 epochs is 0.2004854556446479\n",
      "Cost after 6170 epochs is 0.20283257172390676\n",
      "Cost after 6180 epochs is 0.2046159706343859\n",
      "Cost after 6190 epochs is 0.20531415305308395\n",
      "Cost after 6200 epochs is 0.20288781357350705\n",
      "Cost after 6210 epochs is 0.2020851947057073\n",
      "Cost after 6220 epochs is 0.20057942853345273\n",
      "Cost after 6230 epochs is 0.2007286851118223\n",
      "Cost after 6240 epochs is 0.2018911856008457\n",
      "Cost after 6250 epochs is 0.20336292130489508\n",
      "Cost after 6260 epochs is 0.20306104334542857\n",
      "Cost after 6270 epochs is 0.20061348726228964\n",
      "Cost after 6280 epochs is 0.19922673231681728\n",
      "Cost after 6290 epochs is 0.19742444598975234\n",
      "Cost after 6300 epochs is 0.1988977645511954\n",
      "Cost after 6310 epochs is 0.19846494035011586\n",
      "Cost after 6320 epochs is 0.1971002680398923\n",
      "Cost after 6330 epochs is 0.19884468063196184\n",
      "Cost after 6340 epochs is 0.20043851563757434\n",
      "Cost after 6350 epochs is 0.20422803202872336\n",
      "Cost after 6360 epochs is 0.20211218869854994\n",
      "Cost after 6370 epochs is 0.2023190322002486\n",
      "Cost after 6380 epochs is 0.20318466913620284\n",
      "Cost after 6390 epochs is 0.20203108122305294\n",
      "Cost after 6400 epochs is 0.20308535334104366\n",
      "Cost after 6410 epochs is 0.20146243902187694\n",
      "Cost after 6420 epochs is 0.19924276041898745\n",
      "Cost after 6430 epochs is 0.1966081548521083\n",
      "Cost after 6440 epochs is 0.1970363390559985\n",
      "Cost after 6450 epochs is 0.19850999744189454\n",
      "Cost after 6460 epochs is 0.19748198905539344\n",
      "Cost after 6470 epochs is 0.1978045097394216\n",
      "Cost after 6480 epochs is 0.19771742623179928\n",
      "Cost after 6490 epochs is 0.19832927818835294\n",
      "Cost after 6500 epochs is 0.1981635432953485\n",
      "Cost after 6510 epochs is 0.19723438697587486\n",
      "Cost after 6520 epochs is 0.19799226447358773\n",
      "Cost after 6530 epochs is 0.19775645618771384\n",
      "Cost after 6540 epochs is 0.19753686659076009\n",
      "Cost after 6550 epochs is 0.1973296750562762\n",
      "Cost after 6560 epochs is 0.19597536868724255\n",
      "Cost after 6570 epochs is 0.19735344506949595\n",
      "Cost after 6580 epochs is 0.19757161968000883\n",
      "Cost after 6590 epochs is 0.19707605284314586\n",
      "Cost after 6600 epochs is 0.19824981669881184\n",
      "Cost after 6610 epochs is 0.19727408452280892\n",
      "Cost after 6620 epochs is 0.19718444389283568\n",
      "Cost after 6630 epochs is 0.19702120103924456\n",
      "Cost after 6640 epochs is 0.19755848048472854\n",
      "Cost after 6650 epochs is 0.19744151470401095\n",
      "Cost after 6660 epochs is 0.1971960551254828\n",
      "Cost after 6670 epochs is 0.19622161874932414\n",
      "Cost after 6680 epochs is 0.19682798542177482\n",
      "Cost after 6690 epochs is 0.19689354825687544\n",
      "Cost after 6700 epochs is 0.20411166560570704\n",
      "Cost after 6710 epochs is 0.20596042556559288\n",
      "Cost after 6720 epochs is 0.20054881912499478\n",
      "Cost after 6730 epochs is 0.20071785720867952\n",
      "Cost after 6740 epochs is 0.20083084346852276\n",
      "Cost after 6750 epochs is 0.20058092687197715\n",
      "Cost after 6760 epochs is 0.19998987338364585\n",
      "Cost after 6770 epochs is 0.19447143537885955\n",
      "Cost after 6780 epochs is 0.1962103355589715\n",
      "Cost after 6790 epochs is 0.19494382623365075\n",
      "Cost after 6800 epochs is 0.1962038212828343\n",
      "Cost after 6810 epochs is 0.19493004739367595\n",
      "Cost after 6820 epochs is 0.19555509627227466\n",
      "Cost after 6830 epochs is 0.19483030190589026\n",
      "Cost after 6840 epochs is 0.19573311408040442\n",
      "Cost after 6850 epochs is 0.19625776827274644\n",
      "Cost after 6860 epochs is 0.19615254297672993\n",
      "Cost after 6870 epochs is 0.19578278356700585\n",
      "Cost after 6880 epochs is 0.1952682980450143\n",
      "Cost after 6890 epochs is 0.19569443642682866\n",
      "Cost after 6900 epochs is 0.19488294720919522\n",
      "Cost after 6910 epochs is 0.19465526108455683\n",
      "Cost after 6920 epochs is 0.19527899799515883\n",
      "Cost after 6930 epochs is 0.19509495798962725\n",
      "Cost after 6940 epochs is 0.19464408275917464\n",
      "Cost after 6950 epochs is 0.1950715796720015\n",
      "Cost after 6960 epochs is 0.19542133862893335\n",
      "Cost after 6970 epochs is 0.19681132178104652\n",
      "Cost after 6980 epochs is 0.19456403422844992\n",
      "Cost after 6990 epochs is 0.19459504261988608\n",
      "Cost after 7000 epochs is 0.19554901279410658\n",
      "Cost after 7010 epochs is 0.1983771006668004\n",
      "Cost after 7020 epochs is 0.19965083572598613\n",
      "Cost after 7030 epochs is 0.20238545812281217\n",
      "Cost after 7040 epochs is 0.198431992218991\n",
      "Cost after 7050 epochs is 0.1943236746464492\n",
      "Cost after 7060 epochs is 0.1932853325477542\n",
      "Cost after 7070 epochs is 0.1939421690773602\n",
      "Cost after 7080 epochs is 0.19416666335737162\n",
      "Cost after 7090 epochs is 0.19522209429880938\n",
      "Cost after 7100 epochs is 0.1945376784560038\n",
      "Cost after 7110 epochs is 0.1930676380051193\n",
      "Cost after 7120 epochs is 0.19298558359797635\n",
      "Cost after 7130 epochs is 0.19519882043086612\n",
      "Cost after 7140 epochs is 0.19475218486172394\n",
      "Cost after 7150 epochs is 0.1943624050271758\n",
      "Cost after 7160 epochs is 0.19571473819854296\n",
      "Cost after 7170 epochs is 0.19480820048394112\n",
      "Cost after 7180 epochs is 0.1933831057993592\n",
      "Cost after 7190 epochs is 0.1928154067436793\n",
      "Cost after 7200 epochs is 0.19386002866847976\n",
      "Cost after 7210 epochs is 0.19422106397719938\n",
      "Cost after 7220 epochs is 0.19467186853180757\n",
      "Cost after 7230 epochs is 0.1939309009341512\n",
      "Cost after 7240 epochs is 0.19129502973302273\n",
      "Cost after 7250 epochs is 0.19209537681165792\n",
      "Cost after 7260 epochs is 0.19264456975399546\n",
      "Cost after 7270 epochs is 0.19229602595101697\n",
      "Cost after 7280 epochs is 0.1916135849934707\n",
      "Cost after 7290 epochs is 0.1927835044706852\n",
      "Cost after 7300 epochs is 0.19446622241134812\n",
      "Cost after 7310 epochs is 0.19403024815205788\n",
      "Cost after 7320 epochs is 0.1919189665823316\n",
      "Cost after 7330 epochs is 0.19159641648216053\n",
      "Cost after 7340 epochs is 0.19156867088462753\n",
      "Cost after 7350 epochs is 0.19071728852974965\n",
      "Cost after 7360 epochs is 0.1922415533566183\n",
      "Cost after 7370 epochs is 0.19111358255383196\n",
      "Cost after 7380 epochs is 0.19179675470151533\n",
      "Cost after 7390 epochs is 0.19153964873201104\n",
      "Cost after 7400 epochs is 0.19297906890646047\n",
      "Cost after 7410 epochs is 0.19364835499407765\n",
      "Cost after 7420 epochs is 0.19706104915973344\n",
      "Cost after 7430 epochs is 0.1991898637000247\n",
      "Cost after 7440 epochs is 0.1934327629120244\n",
      "Cost after 7450 epochs is 0.19103489412183353\n",
      "Cost after 7460 epochs is 0.1903972343967605\n",
      "Cost after 7470 epochs is 0.19332324351259947\n",
      "Cost after 7480 epochs is 0.1925303983234958\n",
      "Cost after 7490 epochs is 0.19134333942347143\n",
      "Cost after 7500 epochs is 0.1927469028314276\n",
      "Cost after 7510 epochs is 0.19138376120804568\n",
      "Cost after 7520 epochs is 0.1908236348464888\n",
      "Cost after 7530 epochs is 0.19243600638641958\n",
      "Cost after 7540 epochs is 0.1939432821228928\n",
      "Cost after 7550 epochs is 0.1923456103521401\n",
      "Cost after 7560 epochs is 0.19311826213344063\n",
      "Cost after 7570 epochs is 0.1922137251881455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after 7580 epochs is 0.19121938422004545\n",
      "Cost after 7590 epochs is 0.1907675729398208\n",
      "Cost after 7600 epochs is 0.19023257911548871\n",
      "Cost after 7610 epochs is 0.19236363501056702\n",
      "Cost after 7620 epochs is 0.1905791347494583\n",
      "Cost after 7630 epochs is 0.19094251975939067\n",
      "Cost after 7640 epochs is 0.19337445959246644\n",
      "Cost after 7650 epochs is 0.1909261565762265\n",
      "Cost after 7660 epochs is 0.19070766571584621\n",
      "Cost after 7670 epochs is 0.1919460338747451\n",
      "Cost after 7680 epochs is 0.19079818988854808\n",
      "Cost after 7690 epochs is 0.1905501275220329\n",
      "Cost after 7700 epochs is 0.19211298642770758\n",
      "Cost after 7710 epochs is 0.19061385177159942\n",
      "Cost after 7720 epochs is 0.19016316331584196\n",
      "Cost after 7730 epochs is 0.1909318040343908\n",
      "Cost after 7740 epochs is 0.19106288081681005\n",
      "Cost after 7750 epochs is 0.19252066715355173\n",
      "Cost after 7760 epochs is 0.19193556466446116\n",
      "Cost after 7770 epochs is 0.19007401204050253\n",
      "Cost after 7780 epochs is 0.19004070900479667\n",
      "Cost after 7790 epochs is 0.19121363584614903\n",
      "Cost after 7800 epochs is 0.19351935145166638\n",
      "Cost after 7810 epochs is 0.1975901854480867\n",
      "Cost after 7820 epochs is 0.1942750194659716\n",
      "Cost after 7830 epochs is 0.18928661295966256\n",
      "Cost after 7840 epochs is 0.18974902059316873\n",
      "Cost after 7850 epochs is 0.18969253732315927\n",
      "Cost after 7860 epochs is 0.18971604655462987\n",
      "Cost after 7870 epochs is 0.19019605605737505\n",
      "Cost after 7880 epochs is 0.19043038202248155\n",
      "Cost after 7890 epochs is 0.1908275353494652\n",
      "Cost after 7900 epochs is 0.18841208490832437\n",
      "Cost after 7910 epochs is 0.19098149438398573\n",
      "Cost after 7920 epochs is 0.1879511589254907\n",
      "Cost after 7930 epochs is 0.1915230821314704\n",
      "Cost after 7940 epochs is 0.1883548683782248\n",
      "Cost after 7950 epochs is 0.19373063245303346\n",
      "Cost after 7960 epochs is 0.19521670216354076\n",
      "Cost after 7970 epochs is 0.18962558730445034\n",
      "Cost after 7980 epochs is 0.1891851220497701\n",
      "Cost after 7990 epochs is 0.18921116073954342\n",
      "Cost after 8000 epochs is 0.18891821959708982\n",
      "Cost after 8010 epochs is 0.19058612371634018\n",
      "Cost after 8020 epochs is 0.18861719978043723\n",
      "Cost after 8030 epochs is 0.18827433389024767\n",
      "Cost after 8040 epochs is 0.18958627762562005\n",
      "Cost after 8050 epochs is 0.1890592206480474\n",
      "Cost after 8060 epochs is 0.189394425887649\n",
      "Cost after 8070 epochs is 0.1905439880642314\n",
      "Cost after 8080 epochs is 0.1869572036674099\n",
      "Cost after 8090 epochs is 0.189440985269527\n",
      "Cost after 8100 epochs is 0.19118465395006082\n",
      "Cost after 8110 epochs is 0.19002778184856467\n",
      "Cost after 8120 epochs is 0.19031946634288208\n",
      "Cost after 8130 epochs is 0.19274764836925215\n",
      "Cost after 8140 epochs is 0.1979198087713838\n",
      "Cost after 8150 epochs is 0.1924026529008262\n",
      "Cost after 8160 epochs is 0.1886902393241113\n",
      "Cost after 8170 epochs is 0.18951186038960627\n",
      "Cost after 8180 epochs is 0.18948617999956316\n",
      "Cost after 8190 epochs is 0.18775962672488314\n",
      "Cost after 8200 epochs is 0.18887891129986922\n",
      "Cost after 8210 epochs is 0.18834304263880214\n",
      "Cost after 8220 epochs is 0.18848568632110776\n",
      "Cost after 8230 epochs is 0.18840631894055873\n",
      "Cost after 8240 epochs is 0.18654130828757748\n",
      "Cost after 8250 epochs is 0.18972807048982224\n",
      "Cost after 8260 epochs is 0.18921870179149053\n",
      "Cost after 8270 epochs is 0.18808155089015235\n",
      "Cost after 8280 epochs is 0.18843673700026578\n",
      "Cost after 8290 epochs is 0.18805424690867362\n",
      "Cost after 8300 epochs is 0.18894773399513876\n",
      "Cost after 8310 epochs is 0.18608523322276474\n",
      "Cost after 8320 epochs is 0.1887917338393432\n",
      "Cost after 8330 epochs is 0.18913966328561624\n",
      "Cost after 8340 epochs is 0.18973377486877457\n",
      "Cost after 8350 epochs is 0.1875204002838948\n",
      "Cost after 8360 epochs is 0.18723903289634947\n",
      "Cost after 8370 epochs is 0.19054037437518623\n",
      "Cost after 8380 epochs is 0.1934873775180864\n",
      "Cost after 8390 epochs is 0.19411843479355761\n",
      "Cost after 8400 epochs is 0.18783451032469986\n",
      "Cost after 8410 epochs is 0.18762101491113314\n",
      "Cost after 8420 epochs is 0.18563342743745065\n",
      "Cost after 8430 epochs is 0.18621568884070974\n",
      "Cost after 8440 epochs is 0.18835477129890713\n",
      "Cost after 8450 epochs is 0.18793577266174907\n",
      "Cost after 8460 epochs is 0.18584526608917745\n",
      "Cost after 8470 epochs is 0.18465929745547383\n",
      "Cost after 8480 epochs is 0.1891320430736906\n",
      "Cost after 8490 epochs is 0.19007323196016704\n",
      "Cost after 8500 epochs is 0.18802416787248716\n",
      "Cost after 8510 epochs is 0.18785141360526983\n",
      "Cost after 8520 epochs is 0.18720461099598038\n",
      "Cost after 8530 epochs is 0.18757254169221885\n",
      "Cost after 8540 epochs is 0.18825473085775085\n",
      "Cost after 8550 epochs is 0.18827595308668166\n",
      "Cost after 8560 epochs is 0.19045445395746793\n",
      "Cost after 8570 epochs is 0.19074138458906145\n",
      "Cost after 8580 epochs is 0.1880134136193453\n",
      "Cost after 8590 epochs is 0.18507720384980686\n",
      "Cost after 8600 epochs is 0.1873367566757607\n",
      "Cost after 8610 epochs is 0.18682764102025365\n",
      "Cost after 8620 epochs is 0.184106946058108\n",
      "Cost after 8630 epochs is 0.18454367929999324\n",
      "Cost after 8640 epochs is 0.184333815721715\n",
      "Cost after 8650 epochs is 0.18579865318169267\n",
      "Cost after 8660 epochs is 0.18730151598145495\n",
      "Cost after 8670 epochs is 0.18549480384565145\n",
      "Cost after 8680 epochs is 0.1891389997042537\n",
      "Cost after 8690 epochs is 0.1920278498139088\n",
      "Cost after 8700 epochs is 0.19421747628111188\n",
      "Cost after 8710 epochs is 0.1913936553734741\n",
      "Cost after 8720 epochs is 0.1845384205616715\n",
      "Cost after 8730 epochs is 0.18551086734029895\n",
      "Cost after 8740 epochs is 0.18555254316614156\n",
      "Cost after 8750 epochs is 0.18568514044102694\n",
      "Cost after 8760 epochs is 0.18626151174530525\n",
      "Cost after 8770 epochs is 0.18632775625589917\n",
      "Cost after 8780 epochs is 0.18393025659074272\n",
      "Cost after 8790 epochs is 0.18357773671185967\n",
      "Cost after 8800 epochs is 0.18385797739215579\n",
      "Cost after 8810 epochs is 0.18535523906273793\n",
      "Cost after 8820 epochs is 0.18599170747714946\n",
      "Cost after 8830 epochs is 0.1861278842615755\n",
      "Cost after 8840 epochs is 0.18699071368175516\n",
      "Cost after 8850 epochs is 0.18518291895935465\n",
      "Cost after 8860 epochs is 0.18405740974345244\n",
      "Cost after 8870 epochs is 0.1859512374468288\n",
      "Cost after 8880 epochs is 0.1852984909995377\n",
      "Cost after 8890 epochs is 0.18713763878623194\n",
      "Cost after 8900 epochs is 0.1886722894622872\n",
      "Cost after 8910 epochs is 0.19453255740251219\n",
      "Cost after 8920 epochs is 0.18767690223510972\n",
      "Cost after 8930 epochs is 0.18347278186002883\n",
      "Cost after 8940 epochs is 0.18500474900541783\n",
      "Cost after 8950 epochs is 0.18498330104503163\n",
      "Cost after 8960 epochs is 0.18601276098770528\n",
      "Cost after 8970 epochs is 0.18400824995131557\n",
      "Cost after 8980 epochs is 0.18391132013654912\n",
      "Cost after 8990 epochs is 0.18162645370964486\n",
      "Cost after 9000 epochs is 0.18583799780728258\n",
      "Cost after 9010 epochs is 0.18455471258397307\n",
      "Cost after 9020 epochs is 0.1843464504134578\n",
      "Cost after 9030 epochs is 0.18504290319743158\n",
      "Cost after 9040 epochs is 0.1846631204853309\n",
      "Cost after 9050 epochs is 0.1861919511972069\n",
      "Cost after 9060 epochs is 0.18493444754676963\n",
      "Cost after 9070 epochs is 0.18493359598055928\n",
      "Cost after 9080 epochs is 0.1871345290857096\n",
      "Cost after 9090 epochs is 0.18908302267079619\n",
      "Cost after 9100 epochs is 0.19474160859301118\n",
      "Cost after 9110 epochs is 0.1881321111029067\n",
      "Cost after 9120 epochs is 0.1842863406117511\n",
      "Cost after 9130 epochs is 0.1809518019494957\n",
      "Cost after 9140 epochs is 0.18450698378806113\n",
      "Cost after 9150 epochs is 0.18475412265153157\n",
      "Cost after 9160 epochs is 0.18419397858182116\n",
      "Cost after 9170 epochs is 0.18447802064714452\n",
      "Cost after 9180 epochs is 0.18423483463181242\n",
      "Cost after 9190 epochs is 0.184864487694961\n",
      "Cost after 9200 epochs is 0.1835524085728957\n",
      "Cost after 9210 epochs is 0.18264847055284159\n",
      "Cost after 9220 epochs is 0.1806586102631152\n",
      "Cost after 9230 epochs is 0.1852573015582489\n",
      "Cost after 9240 epochs is 0.1838427372891795\n",
      "Cost after 9250 epochs is 0.18768654391343345\n",
      "Cost after 9260 epochs is 0.19290318596536613\n",
      "Cost after 9270 epochs is 0.18967053068288373\n",
      "Cost after 9280 epochs is 0.18837969628302742\n",
      "Cost after 9290 epochs is 0.18309265197890098\n",
      "Cost after 9300 epochs is 0.18214572168945511\n",
      "Cost after 9310 epochs is 0.1814789499479022\n",
      "Cost after 9320 epochs is 0.182843654043378\n",
      "Cost after 9330 epochs is 0.18309096045311202\n",
      "Cost after 9340 epochs is 0.18408828193982466\n",
      "Cost after 9350 epochs is 0.18310431504394578\n",
      "Cost after 9360 epochs is 0.18197905137673176\n",
      "Cost after 9370 epochs is 0.18166555999252057\n",
      "Cost after 9380 epochs is 0.18366474430792415\n",
      "Cost after 9390 epochs is 0.1830738647799767\n",
      "Cost after 9400 epochs is 0.1861170197806509\n",
      "Cost after 9410 epochs is 0.18929822589774323\n",
      "Cost after 9420 epochs is 0.1909789673250502\n",
      "Cost after 9430 epochs is 0.18821336556122753\n",
      "Cost after 9440 epochs is 0.18297912215018367\n",
      "Cost after 9450 epochs is 0.18161333176409142\n",
      "Cost after 9460 epochs is 0.18318754717464028\n",
      "Cost after 9470 epochs is 0.18328059985297734\n",
      "Cost after 9480 epochs is 0.18281539791216375\n",
      "Cost after 9490 epochs is 0.18239805931784048\n",
      "Cost after 9500 epochs is 0.1821838286006783\n",
      "Cost after 9510 epochs is 0.1826873187370064\n",
      "Cost after 9520 epochs is 0.1821992827982419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after 9530 epochs is 0.1829098851239267\n",
      "Cost after 9540 epochs is 0.1831531178006162\n",
      "Cost after 9550 epochs is 0.18338588198080644\n",
      "Cost after 9560 epochs is 0.1824393071286802\n",
      "Cost after 9570 epochs is 0.1819288763061527\n",
      "Cost after 9580 epochs is 0.18322511515372153\n",
      "Cost after 9590 epochs is 0.1843303662308409\n",
      "Cost after 9600 epochs is 0.18228509478288277\n",
      "Cost after 9610 epochs is 0.18286689340201695\n",
      "Cost after 9620 epochs is 0.18190155999632374\n",
      "Cost after 9630 epochs is 0.18236422958861048\n",
      "Cost after 9640 epochs is 0.1828905559545639\n",
      "Cost after 9650 epochs is 0.1831824520060896\n",
      "Cost after 9660 epochs is 0.18392484104595938\n",
      "Cost after 9670 epochs is 0.18455370738888205\n",
      "Cost after 9680 epochs is 0.1878346073915413\n",
      "Cost after 9690 epochs is 0.1909949688110066\n",
      "Cost after 9700 epochs is 0.18597040046178268\n",
      "Cost after 9710 epochs is 0.18006206263758595\n",
      "Cost after 9720 epochs is 0.18263405041925168\n",
      "Cost after 9730 epochs is 0.18269786465900104\n",
      "Cost after 9740 epochs is 0.1818973707832466\n",
      "Cost after 9750 epochs is 0.18118370791656346\n",
      "Cost after 9760 epochs is 0.1792609370602179\n",
      "Cost after 9770 epochs is 0.1801763918833329\n",
      "Cost after 9780 epochs is 0.1861539210231999\n",
      "Cost after 9790 epochs is 0.18406360906860558\n",
      "Cost after 9800 epochs is 0.18227357255611387\n",
      "Cost after 9810 epochs is 0.18088069913635935\n",
      "Cost after 9820 epochs is 0.1829060899693975\n",
      "Cost after 9830 epochs is 0.18195492247501785\n",
      "Cost after 9840 epochs is 0.18283471932905157\n",
      "Cost after 9850 epochs is 0.18360429771784648\n",
      "Cost after 9860 epochs is 0.1839753956277975\n",
      "Cost after 9870 epochs is 0.1853970528395417\n",
      "Cost after 9880 epochs is 0.188657219486418\n",
      "Cost after 9890 epochs is 0.19049975813604397\n",
      "Cost after 9900 epochs is 0.18920318668244607\n",
      "Cost after 9910 epochs is 0.1896963851678113\n",
      "Cost after 9920 epochs is 0.18778534144240155\n",
      "Cost after 9930 epochs is 0.18059218439812919\n",
      "Cost after 9940 epochs is 0.17951036762220837\n",
      "Cost after 9950 epochs is 0.18120615789133404\n",
      "Cost after 9960 epochs is 0.18305842365479566\n",
      "Cost after 9970 epochs is 0.1808772388390086\n",
      "Cost after 9980 epochs is 0.17930698369708337\n",
      "Cost after 9990 epochs is 0.18396538240442814\n",
      "\n",
      "Training finished.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "\n",
    "print(\"\\nStarting training.\\n\")\n",
    "\n",
    "parameters, cost = NN_model(X = X_train,\n",
    "                            Y = y_train,\n",
    "                            input_layer_dims = input_layer_dims,\n",
    "                            num_iterations = 10000,\n",
    "                            learning_rate = 0.005)\n",
    "\n",
    "print(\"\\nTraining finished.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x22b9d3fddc8>]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training error\n",
    "plt.plot(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions with train data\n",
    "y_pred_train = predict(X_train, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.46447897e-01, 9.45659399e-01, 7.59251897e-01, 9.45689543e-01,\n",
       "        8.88486098e-01, 9.39620244e-01, 1.10562780e-02, 1.32105272e-06,\n",
       "        6.24757316e-01, 9.45616326e-01, 5.92787967e-02, 9.45670998e-01,\n",
       "        2.67467203e-06, 9.45685972e-01, 2.00006099e-01, 4.41845269e-03,\n",
       "        9.15748813e-01, 8.86964240e-01, 9.45603905e-01, 8.79641562e-01,\n",
       "        9.45676967e-01, 4.00376683e-01, 9.45640982e-01, 9.45655593e-01,\n",
       "        9.37751082e-01, 9.40012996e-01, 9.45655817e-01, 9.30541919e-01,\n",
       "        6.79177457e-02, 9.45622409e-01, 9.70085108e-03, 3.69532534e-06,\n",
       "        3.18360336e-06, 5.20677362e-01, 9.45682046e-01, 1.24604860e-02,\n",
       "        9.45627350e-01, 2.17736050e-05, 9.45676627e-01, 9.45642279e-01,\n",
       "        9.45612266e-01, 9.41456977e-01, 9.45680886e-01, 6.00839773e-02,\n",
       "        1.96253632e-02, 4.01343059e-01, 1.54330715e-05, 1.81829105e-31,\n",
       "        9.37121729e-01, 9.45663544e-01, 9.33883486e-01, 9.45649777e-01,\n",
       "        2.00548720e-02, 9.45655529e-01, 7.15484695e-03, 9.45681254e-01,\n",
       "        9.30445336e-01, 2.53499742e-15, 9.45599478e-01, 9.45693809e-01,\n",
       "        9.45658434e-01, 9.45649409e-01, 9.45626697e-01, 1.89061432e-05,\n",
       "        1.98448976e-04, 9.45659460e-01, 9.43930492e-01, 1.47935042e-01,\n",
       "        9.31903266e-01, 9.39146762e-01, 2.94225160e-04, 9.45677807e-01,\n",
       "        9.45684623e-01, 9.45698522e-01, 7.28841580e-03, 2.50988926e-08,\n",
       "        9.99548507e-09, 9.45674159e-01, 1.63547615e-01, 9.45674101e-01,\n",
       "        6.55720948e-01, 5.53739246e-02, 9.45690113e-01, 8.11174594e-05,\n",
       "        5.91866364e-01, 6.24384419e-04, 1.32415376e-06, 9.45678989e-01,\n",
       "        8.88152783e-01, 6.72521518e-06, 9.45626759e-01, 9.45691838e-01,\n",
       "        1.20490249e-04, 9.45658665e-01, 3.90246852e-10, 9.45629891e-01,\n",
       "        1.55759376e-01, 9.45678378e-01, 8.51676353e-01, 1.08882566e-02,\n",
       "        9.45625340e-01, 4.25655851e-19, 9.45672084e-01, 1.43345515e-05,\n",
       "        9.45667532e-01, 9.45690920e-01, 9.24321828e-01, 7.25016750e-01,\n",
       "        9.36637410e-01, 1.06503345e-03, 9.28566215e-01, 9.45633756e-01,\n",
       "        4.87972678e-02, 9.45634050e-01, 9.45686748e-01, 1.27455198e-13,\n",
       "        9.23434139e-01, 8.49232682e-01, 9.38907443e-01, 9.23695870e-01,\n",
       "        8.67570554e-01, 9.45616921e-01, 9.45692475e-01, 9.18749356e-01,\n",
       "        4.86888469e-03, 9.45637993e-01, 9.45677959e-01, 9.45696369e-01,\n",
       "        1.79739529e-04, 9.45602205e-01, 9.45677704e-01, 1.32635128e-05,\n",
       "        9.45590477e-01, 9.45616613e-01, 7.89134966e-04, 9.45643452e-01,\n",
       "        9.10591670e-01, 5.74803620e-03, 8.21774592e-01, 9.45647935e-01,\n",
       "        9.32787857e-01, 9.39502515e-01, 7.60637981e-01, 9.45671619e-01,\n",
       "        9.16468392e-01, 9.45620755e-01, 7.99158794e-01, 9.45660340e-01,\n",
       "        9.45646997e-01, 5.64549709e-01, 8.14369390e-01, 9.35192654e-01,\n",
       "        9.45670835e-01, 2.49500168e-18, 9.45663074e-01, 8.06621278e-01,\n",
       "        9.37013734e-01, 2.11528936e-09, 6.03848290e-04, 4.18911151e-02,\n",
       "        2.28508317e-02, 2.42944553e-03, 9.45699999e-01, 3.28922587e-18,\n",
       "        1.48837771e-05, 1.51089933e-02, 9.45694301e-01, 9.07446708e-01,\n",
       "        9.45658014e-01, 9.45626819e-01, 9.45646318e-01, 8.42669132e-01,\n",
       "        3.32921536e-01, 8.89492079e-01, 7.32561795e-01, 2.18876791e-04,\n",
       "        4.35293819e-04, 9.45663951e-01, 1.05587965e-04, 7.80419271e-01,\n",
       "        1.95779662e-04, 9.45686165e-01, 7.48566119e-01, 9.29761954e-01,\n",
       "        8.63021582e-01, 3.88286962e-05, 9.44254903e-01, 1.82615968e-01,\n",
       "        6.67477487e-01, 9.45686523e-01, 9.45693189e-01, 9.28264072e-01,\n",
       "        8.98936018e-01, 9.41880322e-01, 7.87101605e-04, 6.12143625e-01,\n",
       "        1.62217422e-04, 1.09561854e-04, 9.45680775e-01, 9.32037477e-01,\n",
       "        1.03793632e-01, 9.03931320e-01, 5.73258710e-04, 1.96299261e-07,\n",
       "        1.31721687e-16, 9.45633948e-01, 8.77434122e-01, 4.79751005e-02,\n",
       "        8.83990093e-01, 9.45626811e-01, 1.62120388e-01, 9.88590109e-05,\n",
       "        9.45665983e-01, 9.24612809e-01, 9.45680858e-01, 9.27519144e-01,\n",
       "        9.45655006e-01, 7.89215323e-01, 9.15207071e-01, 9.40649353e-01,\n",
       "        9.45609426e-01, 5.85024431e-07, 4.75160395e-01, 7.50870278e-01,\n",
       "        1.52194749e-05, 7.15276206e-01, 9.45656437e-01, 9.45660129e-01,\n",
       "        9.45663901e-01, 9.25839963e-01, 3.23296466e-01, 8.68496974e-01,\n",
       "        4.45415115e-01, 2.94798247e-05, 9.45678164e-01, 1.70081315e-07,\n",
       "        9.18021165e-01, 5.45376928e-01, 9.45628662e-01, 8.72899540e-01,\n",
       "        9.45637403e-01, 1.29033194e-01, 1.60581840e-04, 1.36707414e-09,\n",
       "        9.27196000e-01, 6.72906955e-01, 9.45651764e-01, 9.45658467e-01,\n",
       "        8.52647493e-01, 9.30235525e-01, 1.66296326e-01, 9.30851114e-01,\n",
       "        9.45633606e-01, 7.92668870e-01, 9.16932612e-01, 8.32259386e-11,\n",
       "        6.87317090e-01, 9.45615268e-01, 2.46309852e-05, 9.45651464e-01,\n",
       "        9.32264917e-01, 6.69857664e-06, 9.45632915e-01, 8.71471094e-01,\n",
       "        9.30486565e-01, 4.43209142e-17, 6.69569861e-10, 6.98022434e-01,\n",
       "        9.45666459e-01, 9.28398039e-01, 9.45672706e-01, 9.45674887e-01,\n",
       "        5.90207820e-02, 9.45673148e-01, 9.45686738e-01, 5.78888885e-07,\n",
       "        8.42034573e-01, 9.38859548e-01, 9.45649024e-01, 9.45689256e-01,\n",
       "        9.45695974e-01, 9.45613572e-01, 9.45656947e-01, 9.41332290e-01,\n",
       "        8.42941780e-12, 1.05742675e-01, 9.14125454e-01, 9.45685037e-01,\n",
       "        9.40095409e-01, 1.97342447e-04, 9.21883198e-01, 9.45649617e-01,\n",
       "        9.35257083e-01, 9.44426194e-01, 2.16659883e-03, 8.96604488e-01,\n",
       "        9.45643066e-01, 9.45637050e-01, 6.37889086e-02, 1.61200769e-13,\n",
       "        9.18096790e-01, 2.79621312e-02, 7.90884445e-01, 9.32137191e-01,\n",
       "        9.45629887e-01, 1.29377632e-07, 9.45600839e-01, 8.72024239e-05,\n",
       "        9.45603613e-01, 9.45633849e-01, 6.23839711e-01, 4.66209286e-02,\n",
       "        9.45660807e-01, 9.45653830e-01, 9.26894358e-01, 3.16397375e-01,\n",
       "        9.28430910e-01, 9.45587771e-01, 8.88129177e-01, 9.32840965e-01,\n",
       "        5.24829464e-01, 2.30935355e-15, 1.02748037e-06, 4.57351975e-01,\n",
       "        9.33650334e-01, 9.45635276e-01, 7.63514461e-03, 8.05956232e-01,\n",
       "        9.45707314e-01, 1.09200815e-01, 1.41277525e-05, 7.32179371e-04,\n",
       "        9.41946163e-01, 9.03983941e-01, 8.98446497e-01, 9.19842153e-01,\n",
       "        9.04203369e-01, 3.23073711e-09, 9.45691983e-01, 9.43876608e-01,\n",
       "        6.63976285e-03, 9.45681374e-01, 9.45673118e-01, 9.45647281e-01,\n",
       "        2.63228663e-01, 9.45677925e-01, 9.33530467e-01, 9.45596768e-01,\n",
       "        6.52376314e-04, 9.08440763e-01, 9.45661916e-01, 8.90903905e-01,\n",
       "        9.45683111e-01, 9.45648751e-01, 9.45683056e-01, 9.43939474e-01,\n",
       "        9.45627676e-01, 5.04050271e-01, 3.84376326e-24, 5.40321009e-01,\n",
       "        8.14430230e-04, 5.99859250e-08, 7.94949025e-04, 9.31189364e-01,\n",
       "        9.45655008e-01, 8.52494320e-01, 9.00616110e-01, 3.43216704e-17,\n",
       "        9.31304996e-01, 8.86837787e-01, 9.45672195e-01, 9.45595222e-01,\n",
       "        7.68780995e-01, 9.34787081e-01, 9.08453546e-01, 9.45670391e-01,\n",
       "        6.64897436e-09, 3.15490525e-02, 8.29551473e-01, 1.99822067e-02,\n",
       "        7.70961159e-05, 9.45610751e-01, 4.85402004e-05, 9.45689770e-01,\n",
       "        9.45626345e-01, 9.45671722e-01, 7.53275046e-11, 9.45675606e-01,\n",
       "        4.73465293e-04, 9.45658388e-01, 8.33058204e-01, 9.45595544e-01,\n",
       "        5.16896190e-10, 7.27487307e-04, 2.02785110e-06, 9.45660411e-01,\n",
       "        9.25077834e-01, 9.45637840e-01, 8.98471305e-01, 1.61800384e-09,\n",
       "        9.45661904e-01, 2.73545149e-02, 9.22488200e-01, 9.45679911e-01,\n",
       "        9.45661785e-01, 9.45594869e-01, 8.93330134e-01, 9.25329475e-01,\n",
       "        9.45088421e-01, 8.91837886e-01, 4.21449252e-09, 9.09128341e-01,\n",
       "        9.05634568e-01, 7.60113706e-02, 9.42357509e-01, 1.25904539e-05,\n",
       "        1.79833252e-02, 9.27799762e-01, 9.45604458e-01, 9.45610206e-01,\n",
       "        9.45608920e-01, 9.45686022e-01, 9.43179779e-01, 4.27261045e-02,\n",
       "        9.42003629e-01, 6.10790946e-16, 9.16579577e-01, 9.45668718e-01,\n",
       "        9.45627536e-01, 9.45680149e-01, 9.45642160e-01, 8.52778286e-01,\n",
       "        3.22146199e-03, 2.69268386e-13, 9.32642108e-01, 9.45598878e-01,\n",
       "        9.17060787e-01, 9.12839341e-01, 9.45661490e-01, 9.45668435e-01,\n",
       "        9.45575052e-01, 8.58908692e-01, 9.45643401e-01, 9.45667810e-01,\n",
       "        9.45672919e-01, 9.45621659e-01, 9.45681553e-01, 9.28032306e-01,\n",
       "        3.20108472e-01, 1.91734107e-05, 1.06817490e-01, 9.32357873e-01,\n",
       "        9.38318475e-01, 1.27763976e-05, 7.73207415e-03, 9.42075774e-01,\n",
       "        6.46691081e-01, 8.28889330e-01, 4.59561565e-04, 6.21127654e-03,\n",
       "        8.99750970e-01, 9.45653516e-01, 9.06807970e-01, 8.80461928e-06,\n",
       "        9.45665139e-01, 9.45676253e-01, 9.45670009e-01, 9.45661097e-01,\n",
       "        9.39176381e-01, 9.45699588e-01, 1.47304887e-02, 7.26514183e-01,\n",
       "        9.17418457e-01, 1.10966108e-05, 8.88441364e-01, 7.95263778e-01,\n",
       "        9.45658585e-01, 8.72079770e-14, 2.90169326e-09, 6.80725428e-06,\n",
       "        8.21648093e-06, 8.26159247e-01, 3.69600134e-01]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize predictions\n",
    "y_pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust shape of prediction and true value\n",
    "y_pred_train = y_pred_train.reshape(-1)\n",
    "y_train = y_train.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True, False, False,  True,\n",
       "        True, False,  True, False,  True, False, False,  True,  True,\n",
       "        True,  True,  True, False,  True,  True,  True,  True,  True,\n",
       "        True, False,  True, False, False, False,  True,  True, False,\n",
       "        True, False,  True,  True,  True,  True,  True, False, False,\n",
       "       False, False, False,  True,  True,  True,  True, False,  True,\n",
       "       False,  True,  True, False,  True,  True,  True,  True,  True,\n",
       "       False, False,  True,  True, False,  True,  True, False,  True,\n",
       "        True,  True, False, False, False,  True, False,  True,  True,\n",
       "       False,  True, False,  True, False, False,  True,  True, False,\n",
       "        True,  True, False,  True, False,  True, False,  True,  True,\n",
       "       False,  True, False,  True, False,  True,  True,  True,  True,\n",
       "        True, False,  True,  True, False,  True,  True, False,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "        True,  True, False,  True,  True, False,  True,  True, False,\n",
       "        True,  True, False,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "       False,  True,  True,  True, False, False, False, False, False,\n",
       "        True, False, False, False,  True,  True,  True,  True,  True,\n",
       "        True, False,  True,  True, False, False,  True, False,  True,\n",
       "       False,  True,  True,  True,  True, False,  True, False,  True,\n",
       "        True,  True,  True,  True,  True, False,  True, False, False,\n",
       "        True,  True, False,  True, False, False, False,  True,  True,\n",
       "       False,  True,  True, False, False,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True, False, False,  True, False,\n",
       "        True,  True,  True,  True,  True, False,  True, False, False,\n",
       "        True, False,  True,  True,  True,  True,  True, False, False,\n",
       "       False,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "        True,  True,  True, False,  True,  True, False,  True,  True,\n",
       "       False,  True,  True,  True, False, False,  True,  True,  True,\n",
       "        True,  True, False,  True,  True, False,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True, False, False,  True,  True,\n",
       "        True, False,  True,  True,  True,  True, False,  True,  True,\n",
       "        True, False, False,  True, False,  True,  True,  True, False,\n",
       "        True, False,  True,  True,  True, False,  True,  True,  True,\n",
       "       False,  True,  True,  True,  True,  True, False, False, False,\n",
       "        True,  True, False,  True,  True, False, False, False,  True,\n",
       "        True,  True,  True,  True, False,  True,  True, False,  True,\n",
       "        True,  True, False,  True,  True,  True, False,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "       False, False, False,  True,  True,  True,  True, False,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True, False, False,\n",
       "        True, False, False,  True, False,  True,  True,  True, False,\n",
       "        True, False,  True,  True,  True, False, False, False,  True,\n",
       "        True,  True,  True, False,  True, False,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True, False,  True,  True, False,\n",
       "        True, False, False,  True,  True,  True,  True,  True,  True,\n",
       "       False,  True, False,  True,  True,  True,  True,  True,  True,\n",
       "       False, False,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True, False, False,\n",
       "       False,  True,  True, False, False,  True,  True,  True, False,\n",
       "       False,  True,  True,  True, False,  True,  True,  True,  True,\n",
       "        True,  True, False,  True,  True, False,  True,  True,  True,\n",
       "       False, False, False, False,  True, False])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_train > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert predictions to the binary class value\n",
    "# (0 or 1, using as threshold 0.5)\n",
    "y_pred_train = 1 * (y_pred_train > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,\n",
       "       1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,\n",
       "       1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,\n",
       "       1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "       0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "       1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,\n",
       "       1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia with training data: 92.96066252587993\n"
     ]
    }
   ],
   "source": [
    "acc_train = sum(1 * (y_pred_train == y_train)) / len(y_pred_train) * 100\n",
    "print(\"Accuracy with training data: \" + str(acc_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Malign       0.97      0.84      0.90       178\n",
      "      Benign       0.91      0.98      0.95       305\n",
      "\n",
      "    accuracy                           0.93       483\n",
      "   macro avg       0.94      0.91      0.92       483\n",
      "weighted avg       0.93      0.93      0.93       483\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train, y_pred_train, target_names = ['Malign', 'Benign']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.45679621e-01, 9.32862709e-01, 7.32943968e-03, 2.02822816e-08,\n",
       "        4.40478262e-03, 1.03478793e-04, 6.09463820e-05, 9.45665645e-01,\n",
       "        9.45662210e-01, 8.51463867e-01, 6.44837080e-01, 9.33128971e-01,\n",
       "        9.45621619e-01, 9.45686627e-01, 9.45673741e-01, 1.56495043e-02,\n",
       "        8.40978787e-01, 6.67483274e-01, 1.30734894e-05, 8.92175157e-01,\n",
       "        9.45693190e-01, 3.79240095e-02, 6.11833102e-11, 9.45662604e-01,\n",
       "        9.45657500e-01, 9.45629778e-01, 8.73860896e-01, 9.45645499e-01,\n",
       "        4.64093678e-14, 4.91663965e-01, 2.96534913e-01, 5.98396196e-01,\n",
       "        9.21411614e-01, 9.45677973e-01, 9.45635353e-01, 8.58003751e-08,\n",
       "        4.27415069e-02, 9.27013592e-01, 9.45634292e-01, 9.45634436e-01,\n",
       "        9.45679158e-01, 1.11537824e-07, 9.45658846e-01, 9.45628788e-01,\n",
       "        9.09441287e-01, 9.44696648e-01, 5.77476864e-02, 9.31413889e-01,\n",
       "        1.30412251e-01, 9.45652566e-01, 9.45655297e-01, 3.79887506e-05,\n",
       "        8.74328901e-09, 9.45639397e-01, 2.74220496e-03, 1.10243005e-01,\n",
       "        9.44729023e-01, 8.45207731e-01, 3.94172878e-01, 1.50883970e-01,\n",
       "        9.45630000e-01, 9.45683261e-01, 8.57697755e-02, 1.13693217e-01,\n",
       "        2.68639571e-07, 8.12546251e-01, 9.42640778e-01, 1.29042309e-01,\n",
       "        8.82552857e-01, 8.52465830e-01, 7.49945743e-31, 9.28042291e-01,\n",
       "        9.45691039e-01, 9.45673006e-01, 9.45600482e-01, 9.45660588e-01,\n",
       "        4.75161328e-03, 1.91171735e-06, 9.45595313e-01, 9.45691501e-01,\n",
       "        9.15008925e-01, 9.45643369e-01, 1.19366452e-01, 9.45690000e-01,\n",
       "        1.44001731e-01, 8.86456800e-01]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predictions with test data\n",
    "y_pred_test = predict(X_test, parameters)\n",
    "y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust shapes\n",
    "y_pred_test = y_pred_test.reshape(-1)\n",
    "y_test = y_test.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,\n",
       "       0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,\n",
       "       1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to binary\n",
    "y_pred_test = 1 * (y_pred_test > 0.5)\n",
    "y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia with test data: 97.67441860465115\n"
     ]
    }
   ],
   "source": [
    "acc_test = sum(1 * (y_pred_test == y_test)) / len(y_pred_test) * 100\n",
    "print(\"Accuracy with test data: \" + str(acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Malign       1.00      0.94      0.97        34\n",
      "      Benign       0.96      1.00      0.98        52\n",
      "\n",
      "    accuracy                           0.98        86\n",
      "   macro avg       0.98      0.97      0.98        86\n",
      "weighted avg       0.98      0.98      0.98        86\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_test, target_names = ['Malign', 'Benign']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
